{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net Segmentation - Camus Dataset\n",
    "\n",
    "*Made by **Hang Jung Ling** and **Olivier Bernard** from the INSA Lyon, France.*\n",
    "\n",
    "This notebook shows how to train, test and evaluate a U-Net to segment different cardiac structures on [CAMUS dataset](https://humanheart-project.creatis.insa-lyon.fr/database/#collection/6373703d73e9f0047faa1bc8).\n",
    "\n",
    "CAMUS is one of the largest public echocardiogaphic datasets, with 500 patients and each patient has 4 echocardiographic images: end-diastolic (ED) and end-systolic (ES) frames acquired in both apical two chamber and apical four chamber views. Each image is annotated by an expert and contains 3 classes + background:</br>\n",
    "&emsp;1) Left ventricle</br>\n",
    "&emsp;2) Myocardium</br>\n",
    "&emsp;3) Left atrium</br>\n",
    "\n",
    "Summary :</br>\n",
    "&emsp;I.   [Install dependencies](#install)</br>\n",
    "&emsp;II.  [Dataset](#dataset)</br>\n",
    "&emsp;II.  [Train](#train)</br>\n",
    "&emsp;III. [Visualize learning curves and predictions](#visualize)</br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Install dependencies <a class=\"anchor\" id=\"install\"></a>\n",
    "\n",
    "Kindly ignore this step if you have installed your own environment using `environment.yaml`. If not, please execute the following cells to install the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture project_path_setup\n",
    "\n",
    "import sys\n",
    "\n",
    "if \"../\" in sys.path:\n",
    "    print(sys.path)\n",
    "else:\n",
    "    sys.path.append(\"../\")\n",
    "    print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture packages_install\n",
    "\n",
    "# Make sure the repo's package and its dependencies are installed\n",
    "%pip install -e ../."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Dataset <a class=\"anchor\" id=\"dataset\"></a>\n",
    "\n",
    "Once the environment is successfully setup, download the CAMUS dataset by executing the following cell. The dataset will be downloaded to the `data/` folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Make sure the data is downloaded and extracted where it should be\n",
    "if not Path(\"../data/camus_64\").is_dir():\n",
    "    import zipfile\n",
    "    from io import BytesIO\n",
    "    from urllib.request import urlopen\n",
    "\n",
    "    zipurl = \"https://www.creatis.insa-lyon.fr/~bernard/camus/camus_64.zip\"\n",
    "    with urlopen(zipurl) as zipresp:\n",
    "        with zipfile.ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "            for member in tqdm(\n",
    "                zfile.infolist(), desc=\"Downloading and extracting data\", position=0, leave=True\n",
    "            ):\n",
    "                try:\n",
    "                    zfile.extract(member, \"../data/\")\n",
    "                except zipfile.error as e:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's split these data into training, validation and testing sets. We will use 80% of the data for training, 10% for validation and 10% for testing. The split is done by patient ID, so that the same patient will not appear in different sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.utils.file_and_folder_operations import subdirs\n",
    "\n",
    "# Specify the data directory\n",
    "data_dir = Path(\"../data/camus_64\").resolve()\n",
    "\n",
    "# List all the patients id\n",
    "keys = subdirs(data_dir, prefix=\"patient\", join=False)\n",
    "\n",
    "# Split the patients into 80/10/10 train/val/test sets\n",
    "train_keys, val_and_test_keys = train_test_split(keys, train_size=0.8, random_state=12345)\n",
    "val_keys, test_keys = train_test_split(val_and_test_keys, test_size=0.5, random_state=12345)\n",
    "\n",
    "train_keys = sorted(train_keys)\n",
    "val_keys = sorted(val_keys)\n",
    "test_keys = sorted(test_keys)\n",
    "\n",
    "# Create train, val and test datalist\n",
    "viws_instants = [\"2CH_ED\", \"2CH_ES\", \"4CH_ED\", \"4CH_ES\"]\n",
    "train_datalist = [\n",
    "    {\n",
    "        \"image\": str(data_dir / key / f\"{key}_{view}.nii.gz\"),\n",
    "        \"label\": str(data_dir / key / f\"{key}_{view}_gt.nii.gz\"),\n",
    "    }\n",
    "    for key in train_keys\n",
    "    for view in viws_instants\n",
    "]\n",
    "\n",
    "val_datalist = [\n",
    "    {\n",
    "        \"image\": str(data_dir / key / f\"{key}_{view}.nii.gz\"),\n",
    "        \"label\": str(data_dir / key / f\"{key}_{view}_gt.nii.gz\"),\n",
    "    }\n",
    "    for key in val_keys\n",
    "    for view in viws_instants\n",
    "]\n",
    "\n",
    "test_datalist = [\n",
    "    {\n",
    "        \"image\": str(data_dir / key / f\"{key}_{view}.nii.gz\"),\n",
    "        \"label\": str(data_dir / key / f\"{key}_{view}_gt.nii.gz\"),\n",
    "    }\n",
    "    for key in test_keys\n",
    "    for view in viws_instants\n",
    "]\n",
    "\n",
    "print(\"Example of train keys: \", train_datalist[:2])\n",
    "print(\"Example of validation keys: \", val_datalist[:2])\n",
    "print(\"Example of test keys: \", test_datalist[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is split, we will create a `Dataset` object for each set. This object will be used to load the data during training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from monai.data import CacheDataset\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    EnsureChannelFirstd,\n",
    "    LoadImaged,\n",
    "    NormalizeIntensityd,\n",
    "    RandAdjustContrastd,\n",
    "    RandFlipd,\n",
    "    RandGaussianNoised,\n",
    "    RandGaussianSmoothd,\n",
    "    RandRotated,\n",
    "    RandScaleIntensityd,\n",
    "    RandZoomd,\n",
    ")\n",
    "\n",
    "# Transforms to load data\n",
    "load_transforms = [\n",
    "    LoadImaged(keys=[\"image\", \"label\"], image_only=True),  # Load image and label\n",
    "    EnsureChannelFirstd(\n",
    "        keys=[\"image\", \"label\"]\n",
    "    ),  # Make sure the first dimension is the channel dimension\n",
    "    NormalizeIntensityd(keys=[\"image\"]),  # Normalize the intensity of the image\n",
    "]\n",
    "\n",
    "# Transforms to augment data\n",
    "range_x = [-15.0 / 180 * np.pi, 15.0 / 180 * np.pi]\n",
    "data_augmentation_transforms = [\n",
    "    RandRotated(\n",
    "        keys=[\"image\", \"label\"],\n",
    "        range_x=range_x,\n",
    "        range_y=0,\n",
    "        range_z=0,\n",
    "        mode=[\"bicubic\", \"nearest\"],\n",
    "        padding_mode=\"zeros\",\n",
    "        prob=0.2,\n",
    "    ),\n",
    "    RandZoomd(\n",
    "        keys=[\"image\", \"label\"],\n",
    "        min_zoom=0.7,\n",
    "        max_zoom=1.4,\n",
    "        mode=[\"bicubic\", \"nearest\"],\n",
    "        padding_mode=\"constant\",\n",
    "        align_corners=(True, None),\n",
    "        prob=0.2,\n",
    "    ),\n",
    "    RandGaussianNoised(keys=[\"image\"], std=0.01, prob=0.15),\n",
    "    RandGaussianSmoothd(\n",
    "        keys=[\"image\"],\n",
    "        sigma_x=(0.5, 1.15),\n",
    "        sigma_y=(0.5, 1.15),\n",
    "        prob=0.15,\n",
    "    ),\n",
    "    RandScaleIntensityd(keys=[\"image\"], factors=0.3, prob=0.15),\n",
    "    RandAdjustContrastd(keys=[\"image\"], gamma=(0.7, 1.5), prob=0.3),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], spatial_axis=[0], prob=0.5),\n",
    "]\n",
    "\n",
    "# Define transforms for training, validation and testing\n",
    "train_transforms = Compose(load_transforms + data_augmentation_transforms)\n",
    "val_transforms = Compose(load_transforms)\n",
    "test_transforms = Compose(load_transforms)\n",
    "\n",
    "# Use CacheDataset to accelerate training and validation\n",
    "train_ds = CacheDataset(data=train_datalist, transform=train_transforms, cache_rate=1.0)\n",
    "val_ds = CacheDataset(data=val_datalist, transform=val_transforms, cache_rate=1.0)\n",
    "test_ds = CacheDataset(data=test_datalist, transform=test_transforms, cache_rate=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's visualize some images from the training set. The images are displayed with their corresponding ground truth segmentation masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from src.utils.visualizations import imagesc\n",
    "\n",
    "# Get a random index to display the image with label from each dataset\n",
    "train_idx = np.random.randint(len(train_ds))\n",
    "val_idx = np.random.randint(len(val_ds))\n",
    "test_idx = np.random.randint(len(test_ds))\n",
    "\n",
    "# Print the selected indices\n",
    "print(\"train_idx: \", train_idx)\n",
    "print(\"val_idx: \", val_idx)\n",
    "print(\"test_idx: \", test_idx)\n",
    "\n",
    "# Visualize a random image with label from each dataset\n",
    "colors = [\"black\", \"red\", \"green\", \"blue\"]\n",
    "cmap = ListedColormap(colors)\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "train_sample = train_ds[train_idx]\n",
    "image = train_sample[\"image\"].detach().cpu().numpy()[0].transpose(1, 0)\n",
    "label = train_sample[\"label\"].detach().cpu().numpy()[0].transpose(1, 0)\n",
    "ax = figure.add_subplot(3, 2, 1)\n",
    "imagesc(ax, image, title=\"Training image\", show_colorbar=False)\n",
    "ax = figure.add_subplot(3, 2, 2)\n",
    "imagesc(\n",
    "    ax, label, title=\"Training label\", show_colorbar=False, colormap=cmap, interpolation=\"nearest\"\n",
    ")\n",
    "\n",
    "val_sample = val_ds[val_idx]\n",
    "image = val_sample[\"image\"].detach().cpu().numpy()[0].transpose(1, 0)\n",
    "label = val_sample[\"label\"].detach().cpu().numpy()[0].transpose(1, 0)\n",
    "ax = figure.add_subplot(3, 2, 3)\n",
    "imagesc(ax, image, title=\"Validation image\", show_colorbar=False)\n",
    "ax = figure.add_subplot(3, 2, 4)\n",
    "imagesc(\n",
    "    ax,\n",
    "    label,\n",
    "    title=\"Validation label\",\n",
    "    show_colorbar=False,\n",
    "    colormap=cmap,\n",
    "    interpolation=\"nearest\",\n",
    ")\n",
    "\n",
    "test_sample = test_ds[test_idx]\n",
    "image = test_sample[\"image\"].detach().cpu().numpy()[0].transpose(1, 0)\n",
    "label = test_sample[\"label\"].detach().cpu().numpy()[0].transpose(1, 0)\n",
    "ax = figure.add_subplot(3, 2, 5)\n",
    "imagesc(ax, image, title=\"Test image\", show_colorbar=False)\n",
    "ax = figure.add_subplot(3, 2, 6)\n",
    "imagesc(ax, label, title=\"Test label\", show_colorbar=False, colormap=cmap, interpolation=\"nearest\")\n",
    "figure.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Train <a class=\"anchor\" id=\"train\"></a>\n",
    "Let's move on to train a U-Net to segment the left ventricle, myocardium and left atrium. We will use the training and validation sets created in the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of U-Net architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "from src.models.unet import UNet\n",
    "\n",
    "input_channels = 1  # This is the number of input channels in the image\n",
    "input_shape = (input_channels, 64, 64)  # This is the shape of the input image to the network\n",
    "num_classes = 4  # This is the number of output classes\n",
    "output_shape = (num_classes, 64, 64)  # This is the shape of the output mask\n",
    "init_channels = 32  # This is the number of channels in the first layer of the network\n",
    "\n",
    "unet = UNet(input_shape=input_shape, output_shape=output_shape, init_channels=init_channels)\n",
    "\n",
    "# Print the summary of the network\n",
    "summary_kwargs = dict(\n",
    "    col_names=[\"input_size\", \"output_size\", \"kernel_size\", \"num_params\"], depth=3, verbose=0\n",
    ")\n",
    "summary(unet, (1, *input_shape), device=\"cpu\", **summary_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define whole auto-encoder and encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class DoubleConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels),\n",
    "            #nn.Dropout2d(p=0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to modify the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "                DoubleConv(in_channels, out_channels),\n",
    "                #nn.Dropout2d(p=0.2)\n",
    "            )\n",
    "        else:\n",
    "            self.up = nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                #nn.Dropout2d(p=0.2)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.up(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class AE_Conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, init_channel, bilinear=True):\n",
    "        super(AE_Conv, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.init_channel = init_channel\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(in_channels, init_channel)\n",
    "        self.down1 = Down(init_channel, init_channel*2)\n",
    "        self.down2 = Down(init_channel*2, init_channel*4)\n",
    "        self.down3 = Down(init_channel*4, init_channel*8)\n",
    "        #self.down4 = Down(init_channel*8, init_channel*16)\n",
    "        #self.up1 = Up(init_channel*16, init_channel*8, bilinear)\n",
    "        self.up2 = Up(init_channel*8, init_channel*4, bilinear)\n",
    "        self.up3 = Up(init_channel*4, init_channel*2, bilinear)\n",
    "        self.up4 = Up(init_channel*2, init_channel, bilinear)\n",
    "        self.outc = OutConv(init_channel, out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.inc(x)\n",
    "        x = self.down1(x)\n",
    "        x = self.down2(x)\n",
    "        x = self.down3(x)\n",
    "        #x = self.down4(x)\n",
    "        #x = self.up1(x)\n",
    "        x = self.up2(x)\n",
    "        x = self.up3(x)\n",
    "        x = self.up4(x)\n",
    "        x = self.outc(x)\n",
    "        #x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "       for m in self.modules():\n",
    "           if isinstance(m, nn.Conv2d):\n",
    "               nn.init.kaiming_normal_(m.weight)\n",
    "               if m.bias is not None:\n",
    "                   nn.init.zeros_(m.bias)\n",
    "           elif isinstance(m, nn.BatchNorm2d):\n",
    "               nn.init.constant_(m.weight, 1)\n",
    "               nn.init.constant_(m.bias, 0)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, init_channel):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.init_channel = init_channel\n",
    "\n",
    "        self.inc = DoubleConv(in_channels, init_channel)\n",
    "        self.down1 = Down(init_channel, init_channel*2)\n",
    "        self.down2 = Down(init_channel*2, init_channel*4)\n",
    "        self.down3 = Down(init_channel*4, init_channel*8)\n",
    "        #self.down4 = Down(init_channel*8, init_channel*16)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.inc(x)\n",
    "        x = self.down1(x)\n",
    "        x = self.down2(x)\n",
    "        x = self.down3(x)\n",
    "        #x = self.down4(x)\n",
    "        return x\n",
    "    def _initialize_weights(self):\n",
    "       for m in self.modules():\n",
    "           if isinstance(m, nn.Conv2d):\n",
    "               nn.init.kaiming_normal_(m.weight)\n",
    "               if m.bias is not None:\n",
    "                   nn.init.zeros_(m.bias)\n",
    "           elif isinstance(m, nn.BatchNorm2d):\n",
    "               nn.init.constant_(m.weight, 1)\n",
    "               nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get auto-encoder weights and load encoder weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the device to run the model on\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "input_channel = 1  # This is the number of input channels in the image\n",
    "output_channel = 1  # This is the number of output channel\n",
    "init_channels = 32  # This is the output channel's number of input convolution\n",
    "ae_input_channels = 1 # This is the number of auto-encoder input channels in the image\n",
    "\n",
    "ae_conv = AE_Conv(input_channel, output_channel, init_channels)\n",
    "encoder = Encoder(ae_input_channels, init_channels)\n",
    "\n",
    "ae_conv = torch.load(\"../logs/ae_label/best_metric_model.pth\", map_location=device)\n",
    "\n",
    "#Load the 4 first layers parameters of auto-encoder into encoder\n",
    "encoder.inc.load_state_dict(ae_conv.inc.state_dict())\n",
    "encoder.down1.load_state_dict(ae_conv.down1.state_dict())\n",
    "encoder.down2.load_state_dict(ae_conv.down2.state_dict())\n",
    "encoder.down3.load_state_dict(ae_conv.down3.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of optimizer and loss function\n",
    "We will use the Adam optimizer. The loss function is the combination of the Dice and cross-entropy (CE) loss, which is a standard loss function for segmentation tasks.\n",
    "> **&#9432;** The exact computation of the loss function is CE - Dice, which means its minimum value is -1 instead of 0 in the best case scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "from src.utils.loss_functions.dice_loss import DC_and_CE_loss\n",
    "\n",
    "# Soft dice and CE loss function\n",
    "loss_function = DC_and_CE_loss(\n",
    "    {\"batch_dice\": True, \"smooth\": 1e-5, \"do_bg\": False}, weight_ce=1, weight_dice=1\n",
    ")\n",
    "\n",
    "# MSE loss function for comparison between ref (=label) and prediction in the auto-encoder latent space\n",
    "loss_function_latent = MSELoss();\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = partial(torch.optim.Adam, params=unet.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from typing import Union\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from monai.data import DataLoader\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.utils.tensor_utils import sum_tensor\n",
    "\n",
    "\n",
    "def train_process(\n",
    "    train_ds: Dataset,\n",
    "    val_ds: Dataset,\n",
    "    num_workers: int,\n",
    "    model: nn.Module,\n",
    "    encoder: nn.Module,\n",
    "    loss_function: nn.Module,\n",
    "    loss_function_latent: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    num_classes: int,\n",
    "    batch_size: int = 2,\n",
    "    lr: float = 0.001,\n",
    "    max_epochs: int = 30,\n",
    "    log_dir: Union[Path, str] = Path(\"../logs/camus_segmentation\"),\n",
    "    val_interval=1,\n",
    ") -> tuple[float, list[float], list[float], list[int], list[float]]:\n",
    "    \"\"\"Trains a neural network model for segmentation on the provided datasets using the specified parameters.\n",
    "\n",
    "    Args:\n",
    "        train_ds: Training dataset.\n",
    "        val_ds: Validation dataset.\n",
    "        num_workers: Number of workers to use for data loading.\n",
    "        model: Neural network model.\n",
    "        loss_function: Loss function.\n",
    "        optimizer: Optimizer.\n",
    "        num_classes: Number of classes to segment.\n",
    "        batch_size: Number of batch size. Defaults to 2.\n",
    "        lr: Learning rate. Defaults to 0.001.\n",
    "        max_epochs: Maximum training epochs. Defaults to 30.\n",
    "        log_dir: Path to the logging directory. Defaults to Path(\"../logs/camus_segmentation\").\n",
    "        val_interval: Epoch interval to perform evaluation steps. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        - Total time taken for training.\n",
    "        - List containing training loss values for each epoch.\n",
    "        - List containing validation loss values for each evaluation epoch.\n",
    "        - List containing epochs where validation is performed.\n",
    "        - List containing metric values for each evaluation epoch.\n",
    "    \"\"\"\n",
    "    # Create train and validation dataloaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    # Determine the device to run the model on\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"GPU detected, training on: {device}!\\n\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"GPU not detected, training on CPU!\\n\")\n",
    "\n",
    "    # Move the models to the device\n",
    "    model = model.to(device)\n",
    "    encoder = encoder.to(device)\n",
    "\n",
    "    # Finalize the creation of the optimizer object with the learning rate\n",
    "    optimizer = optimizer(lr=lr)\n",
    "\n",
    "    # Convert log directory to Path object if needed\n",
    "    if not isinstance(log_dir, Path):\n",
    "        log_dir = Path(log_dir)\n",
    "    # Create the log directory if it does not exist\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Define some variables to keep track of the best metric values, epoch time and losses\n",
    "    best_metric = -1\n",
    "    best_metric_epoch = -1\n",
    "    epoch_train_loss_values = []\n",
    "    epoch_val_loss_values = []\n",
    "    metric_values = []\n",
    "    epoch_val = []\n",
    "    metric_per_class = {f\"metric/{i}\": [] for i in range(1, num_classes)}\n",
    "    total_start = time.time()\n",
    "    \n",
    "    # Losses weights\n",
    "    k_segm = 0.5\n",
    "    k_latent = 0.5\n",
    "\n",
    "    fit_pbar = tqdm(range(max_epochs), desc=\"Training\", unit=\"epoch\", position=0, leave=True)\n",
    "    pbar_metrics = {\"train/loss\": None, \"val/loss\": None, \"val/dice\": None}\n",
    "\n",
    "    for epoch in fit_pbar:\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_val_loss = 0\n",
    "        step = 0\n",
    "        epoch_tp_hard = []\n",
    "        epoch_fp_hard = []\n",
    "        epoch_fn_hard = []\n",
    "        for batch_data in train_dataloader:\n",
    "            step += 1\n",
    "            inputs, labels = (\n",
    "                batch_data[\"image\"].to(device),\n",
    "                batch_data[\"label\"].to(device),\n",
    "            )\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            #We pass the prediction and the label into the encoder\n",
    "            outputs_1_ch = F.softmax(outputs, dim=1).argmax(dim=1, keepdim=True).float() #we use argmax to have only 1 channel on output\n",
    "            outputs_latent = encoder(outputs_1_ch) #encode segmented image into latent space\n",
    "            labels_latent = encoder(labels) #encode labels (=reference) into latent space\n",
    "            \n",
    "            #We compute both losses\n",
    "            loss_segm = loss_function(outputs, labels)\n",
    "            loss_latent = loss_function_latent(outputs_latent, labels_latent) #compute MSE loss in the latent space\n",
    "            loss = k_segm*loss_segm + k_latent*loss_latent #weighted sum of both losses\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            pbar_metrics[\"loss\"] = loss.item()\n",
    "            pbar_metrics[\"train_batch\"] = f\"{step}/{len(train_dataloader)}\"\n",
    "            fit_pbar.set_postfix(pbar_metrics)\n",
    "\n",
    "        epoch_loss /= step\n",
    "        epoch_train_loss_values.append(epoch_loss)\n",
    "        pbar_metrics[\"train/loss\"] = epoch_loss\n",
    "        pbar_metrics.pop(\"loss\")\n",
    "        pbar_metrics.pop(\"train_batch\")\n",
    "        fit_pbar.set_postfix(pbar_metrics)\n",
    "\n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            step = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for val_data in val_dataloader:\n",
    "                    step += 1\n",
    "                    val_inputs, val_labels = (\n",
    "                        val_data[\"image\"].to(device),\n",
    "                        val_data[\"label\"].to(device),\n",
    "                    )\n",
    "                    val_outputs = model(val_inputs)\n",
    "                    \n",
    "                    #We pass the prediction and the label into the encoder\n",
    "                    val_outputs_1_ch = F.softmax(val_outputs, dim=1).argmax(dim=1, keepdim=True).float() #we use argmax to have only 1 channel on output\n",
    "                    val_outputs_latent = encoder(val_outputs_1_ch) #encode segmented image into latent space\n",
    "                    val_labels_latent = encoder(val_labels) #encode labels (=reference) into latent space\n",
    "\n",
    "                    #We compute both losses\n",
    "                    val_loss_segm = loss_function(outputs, labels)\n",
    "                    val_loss_latent = loss_function_latent(val_outputs_latent, val_labels_latent) #compute MSE loss in the latent space\n",
    "                    val_loss = k_segm*val_loss_segm + k_latent*val_loss_latent #weighted sum of both losses\n",
    "                    \n",
    "                    epoch_val_loss += val_loss.item()\n",
    "                    pbar_metrics[\"loss\"] = val_loss.item()\n",
    "                    pbar_metrics[\"val_batch\"] = f\"{step}/{len(val_dataloader)}\"\n",
    "                    fit_pbar.set_postfix(pbar_metrics)\n",
    "\n",
    "                    # Compute tp, fp, and fn that will be used to compute the hard dice metric per epoch\n",
    "                    val_labels = val_labels[:, 0]\n",
    "                    axes = tuple(range(1, len(val_labels.shape)))\n",
    "                    # Convert the output logits to a segmentation mask\n",
    "                    val_outputs = F.softmax(val_outputs, dim=1).argmax(dim=1)\n",
    "\n",
    "                    tp_hard = torch.zeros((val_labels.shape[0], num_classes - 1)).to(\n",
    "                        val_outputs.device.index\n",
    "                    )\n",
    "                    fp_hard = torch.zeros((val_labels.shape[0], num_classes - 1)).to(\n",
    "                        val_outputs.device.index\n",
    "                    )\n",
    "                    fn_hard = torch.zeros((val_labels.shape[0], num_classes - 1)).to(\n",
    "                        val_outputs.device.index\n",
    "                    )\n",
    "\n",
    "                    for c in range(1, num_classes):\n",
    "                        tp_hard[:, c - 1] = sum_tensor(\n",
    "                            (val_outputs == c).float() * (val_labels == c).float(), axes=axes\n",
    "                        )\n",
    "                        fp_hard[:, c - 1] = sum_tensor(\n",
    "                            (val_outputs == c).float() * (val_labels != c).float(), axes=axes\n",
    "                        )\n",
    "                        fn_hard[:, c - 1] = sum_tensor(\n",
    "                            (val_outputs != c).float() * (val_labels == c).float(), axes=axes\n",
    "                        )\n",
    "\n",
    "                    tp_hard = tp_hard.sum(0, keepdim=False).detach().cpu().numpy()\n",
    "                    fp_hard = fp_hard.sum(0, keepdim=False).detach().cpu().numpy()\n",
    "                    fn_hard = fn_hard.sum(0, keepdim=False).detach().cpu().numpy()\n",
    "\n",
    "                    # Store the tp_hard, fp_hard, and fn_hard per evaluation step\n",
    "                    epoch_tp_hard.append(list(tp_hard))\n",
    "                    epoch_fp_hard.append(list(fp_hard))\n",
    "                    epoch_fn_hard.append(list(fn_hard))\n",
    "\n",
    "                epoch_val_loss /= step\n",
    "                epoch_val_loss_values.append(epoch_val_loss)\n",
    "                epoch_val.append(epoch + 1)\n",
    "                pbar_metrics.pop(\"loss\")\n",
    "                pbar_metrics.pop(\"val_batch\")\n",
    "                pbar_metrics[\"val/loss\"] = float(epoch_val_loss)\n",
    "\n",
    "                # Compute the hard dice metric per epoch\n",
    "                epoch_tp_hard = np.sum(epoch_tp_hard, 0)\n",
    "                epoch_fp_hard = np.sum(epoch_fp_hard, 0)\n",
    "                epoch_fn_hard = np.sum(epoch_fn_hard, 0)\n",
    "\n",
    "                # Compute the hard dice metric per class\n",
    "                global_dc_per_class = [\n",
    "                    i if not np.isnan(i) else 0.0\n",
    "                    for i in [\n",
    "                        2 * i / (2 * i + j + k)\n",
    "                        for i, j, k in zip(epoch_tp_hard, epoch_fp_hard, epoch_fn_hard)\n",
    "                    ]\n",
    "                ]\n",
    "\n",
    "                val_metric = np.mean(global_dc_per_class)\n",
    "                metric_values.append(val_metric)\n",
    "                pbar_metrics[\"val/dice\"] = val_metric\n",
    "                for i in range(1, num_classes, 1):\n",
    "                    pbar_metrics[f\"val/dice/{i}\"] = global_dc_per_class[i - 1]\n",
    "                    metric_per_class[f\"metric/{i}\"].append(global_dc_per_class[i - 1])\n",
    "                fit_pbar.set_postfix(pbar_metrics)\n",
    "\n",
    "                if val_metric > best_metric:\n",
    "                    best_metric = val_metric\n",
    "                    best_metric_epoch = epoch + 1\n",
    "                    # Save best metric model checkpoint\n",
    "                    torch.save(\n",
    "                        {\n",
    "                            \"max_epochs\": max_epochs,\n",
    "                            \"current_epoch\": epoch + 1,\n",
    "                            \"best_metric_epoch\": best_metric_epoch,\n",
    "                            \"train_loss\": epoch_train_loss_values,\n",
    "                            \"val_loss\": epoch_val_loss_values,\n",
    "                            \"epoch_val\": epoch_val,\n",
    "                            \"metric_values\": metric_values,\n",
    "                            \"metric_per_class\": metric_per_class,\n",
    "                            \"model_state_dict\": model.state_dict(),\n",
    "                            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                        },\n",
    "                        str(log_dir / \"best_metric_model.pth\"),\n",
    "                    )\n",
    "\n",
    "        # Save last model checkpoint\n",
    "        torch.save(\n",
    "            {\n",
    "                \"max_epochs\": max_epochs,\n",
    "                \"current_epoch\": epoch + 1,\n",
    "                \"best_metric_epoch\": best_metric_epoch,\n",
    "                \"train_loss\": epoch_train_loss_values,\n",
    "                \"val_loss\": epoch_val_loss_values,\n",
    "                \"epoch_val\": epoch_val,\n",
    "                \"metric_values\": metric_values,\n",
    "                \"metric_per_class\": metric_per_class,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            },\n",
    "            str(log_dir / \"last_model.pth\"),\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        f\"train completed, best_metric: {best_metric:.4f}\"\n",
    "        f\" at epoch: {best_metric_epoch}\"\n",
    "        f\" total time: {(time.time() - total_start):.4f}\"\n",
    "    )\n",
    "    return (\n",
    "        time.time() - total_start,\n",
    "        epoch_train_loss_values,\n",
    "        epoch_val_loss_values,\n",
    "        epoch_val,\n",
    "        metric_values,\n",
    "        metric_per_class,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of training hyperparameters\n",
    "In this section, we will define the hyperparameters for training, such as the number of epochs, the learning rate, the batch size, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2  # Number of batch size\n",
    "lr = 0.001  # Learning rate\n",
    "max_epochs = 10  # Number of epochs to train the model\n",
    "num_workers = os.cpu_count() - 1  # Number of workers to use for data loading\n",
    "\n",
    "# Train the model\n",
    "(\n",
    "    total_time,\n",
    "    epoch_train_loss_values,\n",
    "    epoch_val_loss_values,\n",
    "    epoch_val,\n",
    "    metric_values,\n",
    "    metric_per_class,\n",
    ") = train_process(\n",
    "    train_ds=train_ds,\n",
    "    val_ds=val_ds,\n",
    "    num_workers=num_workers,\n",
    "    model=unet,\n",
    "    encoder=encoder,\n",
    "    loss_function=loss_function,\n",
    "    loss_function_latent=loss_function_latent,\n",
    "    optimizer=optimizer,\n",
    "    num_classes=num_classes,\n",
    "    batch_size=batch_size,\n",
    "    lr=lr,\n",
    "    max_epochs=max_epochs,\n",
    "    log_dir=\"../logs/camus_segmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Visualize learning curves and predictions <a class=\"anchor\" id=\"visualize\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training losses, validation losses and validation dice over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "trains_epoch = list(range(1, max_epochs + 1, 1))\n",
    "vals_epochs = epoch_val\n",
    "plt.figure(\"train\", (16, 8))\n",
    "ax = plt.subplot(1, 3, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))  # Set integer ticks for x-axis\n",
    "plt.plot(trains_epoch, epoch_train_loss_values, color=\"red\", label=\"Train\")\n",
    "plt.plot(vals_epochs, epoch_val_loss_values, color=\"blue\", label=\"Val\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "ax = plt.subplot(1, 3, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))  # Set integer ticks for x-axis\n",
    "plt.plot(vals_epochs, metric_values, color=\"green\")\n",
    "\n",
    "ax = plt.subplot(1, 3, 3)\n",
    "plt.title(\"Val Mean Dice per Class\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))  # Set integer ticks for x-axis\n",
    "legend_metric = [\"left ventricle\", \"myocardium\", \"left atrium\"]\n",
    "for i in range(1, num_classes, 1):\n",
    "    plt.plot(vals_epochs, metric_per_class[f\"metric/{i}\"], label=legend_metric[i - 1])\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the pre-trained model weight for 50 epochs and visualize learning curves and metrics over 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "# Path to the logging directory\n",
    "log_dir = \"../logs/camus_segmentation\"\n",
    "\n",
    "# Determine the device to run the model on\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Load the best model weight\n",
    "best_model = Path(log_dir) / \"best_model_50epochs.pth\"\n",
    "if not best_model.is_file():\n",
    "    import urllib.request\n",
    "\n",
    "    urllib.request.urlretrieve(\n",
    "        \"https://www.creatis.insa-lyon.fr/~bernard/camus/best_model_50epochs.pth\",\n",
    "        str(best_model),\n",
    "    )\n",
    "\n",
    "# Load the weight\n",
    "weight = torch.load(best_model, map_location=device)\n",
    "epoch_train_loss_values = weight[\"train_loss\"]\n",
    "epoch_val_loss_values = weight[\"val_loss\"]\n",
    "epoch_val = weight[\"epoch_val\"]\n",
    "metric_values = weight[\"metric_values\"]\n",
    "metric_per_class = weight[\"metric_per_class\"]\n",
    "max_epochs = weight[\"max_epochs\"]\n",
    "best_metric_epoch = weight[\"best_metric_epoch\"]\n",
    "\n",
    "# Plot the training losses, validation losses and validation dice over epochs\n",
    "trains_epoch = list(range(1, max_epochs + 1, 1))\n",
    "vals_epochs = epoch_val\n",
    "\n",
    "plt.figure(\"train\", (16, 8))\n",
    "ax = plt.subplot(1, 3, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))  # Set integer ticks for x-axis\n",
    "plt.plot(trains_epoch, epoch_train_loss_values, color=\"red\", label=\"Train\")\n",
    "plt.plot(vals_epochs, epoch_val_loss_values, color=\"blue\", label=\"Val\")\n",
    "# Add a vertical line at the best model epoch\n",
    "plt.axvline(best_metric_epoch, color=\"gray\", linestyle=\"--\")\n",
    "plt.text(\n",
    "    best_metric_epoch + 0.5,\n",
    "    min(epoch_train_loss_values) * 4 / 5,\n",
    "    f\"Best model epoch = {best_metric_epoch}\",\n",
    "    rotation=90,\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "ax = plt.subplot(1, 3, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))  # Set integer ticks for x-axis\n",
    "plt.plot(vals_epochs, metric_values, color=\"green\")\n",
    "plt.axvline(best_metric_epoch, color=\"gray\", linestyle=\"--\")\n",
    "\n",
    "ax = plt.subplot(1, 3, 3)\n",
    "plt.title(\"Val Mean Dice per Class\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))  # Set integer ticks for x-axis\n",
    "legend_metric = [\"left ventricle\", \"myocardium\", \"left atrium\"]\n",
    "for i in range(1, num_classes, 1):\n",
    "    plt.plot(vals_epochs, metric_per_class[f\"metric/{i}\"], label=legend_metric[i - 1])\n",
    "plt.axvline(best_metric_epoch, color=\"gray\", linestyle=\"--\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the pre-trained model weight into the U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True to use the pretrained model\n",
    "# Set to False to use the model you just trained\n",
    "use_pretrained = False\n",
    "\n",
    "if use_pretrained:\n",
    "    weight = torch.load(Path(log_dir) / \"best_model_50epochs.pth\", map_location=device)\n",
    "else:\n",
    "    weight = torch.load(Path(log_dir) / \"best_metric_model.pth\", map_location=device)\n",
    "\n",
    "# Load the weight into the U-Net\n",
    "unet.load_state_dict(weight[\"model_state_dict\"])\n",
    "# Move the U-Net to the correct device\n",
    "unet.to(device)\n",
    "# Put the U-Net in evaluation mode\n",
    "unet.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the inference on the test set\n",
    "First, we create the dataloader from the test dataset we defined at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the test dataloader\n",
    "num_workers = os.cpu_count() - 1\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=1,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# Create an iterator to iterate over the test dataloader\n",
    "test_dataloader_iter = iter(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we perform the inference on a sample from the test dataset and plot the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on a test image\n",
    "batch_data = next(test_dataloader_iter)\n",
    "with torch.no_grad():\n",
    "    image = batch_data[\"image\"].to(device)\n",
    "    pred = unet(image)\n",
    "    pred = F.softmax(pred, dim=1).argmax(dim=1)\n",
    "    pred = pred.squeeze().detach().cpu().numpy().transpose(1, 0)\n",
    "    label = batch_data[\"label\"].squeeze().detach().cpu().numpy().transpose(1, 0)\n",
    "    image = image.squeeze().detach().cpu().numpy().transpose(1, 0)\n",
    "\n",
    "# Plot the image, label and prediction\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "ax = figure.add_subplot(1, 3, 1)\n",
    "imagesc(ax, image, title=\"Image\", show_colorbar=False)\n",
    "ax = figure.add_subplot(1, 3, 2)\n",
    "imagesc(ax, label, title=\"Label\", show_colorbar=False, colormap=cmap, interpolation=\"nearest\")\n",
    "ax = figure.add_subplot(1, 3, 3)\n",
    "imagesc(ax, pred, title=\"Prediction\", show_colorbar=False, colormap=cmap, interpolation=\"nearest\")\n",
    "figure.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
