{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net Segmentation - Camus Dataset\n",
    "\n",
    "*Made by **Hang Jung Ling** and **Olivier Bernard** from the INSA Lyon, France.*\n",
    "\n",
    "This notebook shows how to train, test and evaluate a U-Net to segment different cardiac structures on [CAMUS dataset](https://humanheart-project.creatis.insa-lyon.fr/database/#collection/6373703d73e9f0047faa1bc8).\n",
    "\n",
    "CAMUS is one of the largest public echocardiogaphic datasets, with 500 patients and each patient has 4 echocardiographic images: end-diastolic (ED) and end-systolic (ES) frames acquired in both apical two chamber and apical four chamber views. Each image is annotated by an expert and contains 3 classes + background:</br>\n",
    "&emsp;1) Left ventricle</br>\n",
    "&emsp;2) Myocardium</br>\n",
    "&emsp;3) Left atrium</br>\n",
    "\n",
    "Summary :</br>\n",
    "&emsp;I.   [Install dependencies](#install)</br>\n",
    "&emsp;II.  [Dataset](#dataset)</br>\n",
    "&emsp;II.  [Train](#train)</br>\n",
    "&emsp;III. [Visualize learning curves and predictions](#visualize)</br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Install dependencies <a class=\"anchor\" id=\"install\"></a>\n",
    "\n",
    "Kindly ignore this step if you have installed your own environment using `environment.yaml`. If not, please execute the following cells to install the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture project_path_setup\n",
    "\n",
    "import sys\n",
    "\n",
    "if \"../\" in sys.path:\n",
    "    print(sys.path)\n",
    "else:\n",
    "    sys.path.append(\"../\")\n",
    "    print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture packages_install\n",
    "\n",
    "# Make sure the repo's package and its dependencies are installed\n",
    "%pip install -e ../."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Dataset <a class=\"anchor\" id=\"dataset\"></a>\n",
    "\n",
    "Once the environment is successfully setup, download the CAMUS dataset by executing the following cell. The dataset will be downloaded to the `data/` folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Make sure the data is downloaded and extracted where it should be\n",
    "if not Path(\"../data/camus_64\").is_dir():\n",
    "    import zipfile\n",
    "    from io import BytesIO\n",
    "    from urllib.request import urlopen\n",
    "\n",
    "    zipurl = \"https://www.creatis.insa-lyon.fr/~bernard/camus/camus_64.zip\"\n",
    "    with urlopen(zipurl) as zipresp:\n",
    "        with zipfile.ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "            for member in tqdm(\n",
    "                zfile.infolist(), desc=\"Downloading and extracting data\", position=0, leave=True\n",
    "            ):\n",
    "                try:\n",
    "                    zfile.extract(member, \"../data/\")\n",
    "                except zipfile.error as e:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's split these data into training, validation and testing sets. We will use 80% of the data for training, 10% for validation and 10% for testing. The split is done by patient ID, so that the same patient will not appear in different sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of train keys:  [{'label': '/home/eblaszczyk/Bureau/TDSI/PTI/Auto_Encoder/auto-encoder-PTI/data/camus_64/patient0001/patient0001_2CH_ED_gt.nii.gz'}, {'label': '/home/eblaszczyk/Bureau/TDSI/PTI/Auto_Encoder/auto-encoder-PTI/data/camus_64/patient0001/patient0001_2CH_ES_gt.nii.gz'}]\n",
      "Example of validation keys:  [{'label': '/home/eblaszczyk/Bureau/TDSI/PTI/Auto_Encoder/auto-encoder-PTI/data/camus_64/patient0012/patient0012_2CH_ED_gt.nii.gz'}, {'label': '/home/eblaszczyk/Bureau/TDSI/PTI/Auto_Encoder/auto-encoder-PTI/data/camus_64/patient0012/patient0012_2CH_ES_gt.nii.gz'}]\n",
      "Example of test keys:  [{'label': '/home/eblaszczyk/Bureau/TDSI/PTI/Auto_Encoder/auto-encoder-PTI/data/camus_64/patient0010/patient0010_2CH_ED_gt.nii.gz'}, {'label': '/home/eblaszczyk/Bureau/TDSI/PTI/Auto_Encoder/auto-encoder-PTI/data/camus_64/patient0010/patient0010_2CH_ES_gt.nii.gz'}]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.utils.file_and_folder_operations import subdirs\n",
    "\n",
    "# Specify the data directory\n",
    "data_dir = Path(\"../data/camus_64\").resolve()\n",
    "\n",
    "# List all the patients id\n",
    "keys = subdirs(data_dir, prefix=\"patient\", join=False)\n",
    "\n",
    "# Split the patients into 80/10/10 train/val/test sets\n",
    "train_keys, val_and_test_keys = train_test_split(keys, train_size=0.8, random_state=12345)\n",
    "val_keys, test_keys = train_test_split(val_and_test_keys, test_size=0.5, random_state=12345)\n",
    "\n",
    "train_keys = sorted(train_keys)\n",
    "val_keys = sorted(val_keys)\n",
    "test_keys = sorted(test_keys)\n",
    "\n",
    "# Create train, val and test datalist\n",
    "viws_instants = [\"2CH_ED\", \"2CH_ES\", \"4CH_ED\", \"4CH_ES\"]\n",
    "train_datalist = [\n",
    "    {\n",
    "        #\"image\": str(data_dir / key / f\"{key}_{view}.nii.gz\"),   we only take labels for the auto-encoder\n",
    "        \"label\": str(data_dir / key / f\"{key}_{view}_gt.nii.gz\"),\n",
    "    }\n",
    "    for key in train_keys\n",
    "    for view in viws_instants\n",
    "]\n",
    "\n",
    "val_datalist = [\n",
    "    {\n",
    "        #\"image\": str(data_dir / key / f\"{key}_{view}.nii.gz\"),\n",
    "        \"label\": str(data_dir / key / f\"{key}_{view}_gt.nii.gz\"),\n",
    "    }\n",
    "    for key in val_keys\n",
    "    for view in viws_instants\n",
    "]\n",
    "\n",
    "test_datalist = [\n",
    "    {\n",
    "        #\"image\": str(data_dir / key / f\"{key}_{view}.nii.gz\"),\n",
    "        \"label\": str(data_dir / key / f\"{key}_{view}_gt.nii.gz\"),\n",
    "    }\n",
    "    for key in test_keys\n",
    "    for view in viws_instants\n",
    "]\n",
    "\n",
    "print(\"Example of train keys: \", train_datalist[:2])\n",
    "print(\"Example of validation keys: \", val_datalist[:2])\n",
    "print(\"Example of test keys: \", test_datalist[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is split, we will create a `Dataset` object for each set. This object will be used to load the data during training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libc10_cuda.so: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "Loading dataset: 100%|█████████████████████| 1600/1600 [00:11<00:00, 144.48it/s]\n",
      "Loading dataset: 100%|███████████████████████| 200/200 [00:01<00:00, 149.36it/s]\n",
      "Loading dataset: 100%|███████████████████████| 200/200 [00:01<00:00, 145.01it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from monai.data import CacheDataset\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    EnsureChannelFirstd,\n",
    "    LoadImaged,\n",
    "    NormalizeIntensityd,\n",
    "    RandAdjustContrastd,\n",
    "    RandFlipd,\n",
    "    RandGaussianNoised,\n",
    "    RandGaussianSmoothd,\n",
    "    RandRotated,\n",
    "    RandScaleIntensityd,\n",
    "    RandZoomd,\n",
    ")\n",
    "\n",
    "# Transforms to load data\n",
    "load_transforms = [\n",
    "    #LoadImaged(keys=[\"image\", \"label\"], image_only=True),  # Load image and label\n",
    "    #EnsureChannelFirstd(\n",
    "    #    keys=[\"image\", \"label\"]\n",
    "    #),  # Make sure the first dimension is the channel dimension\n",
    "    #NormalizeIntensityd(keys=[\"image\"]),  # Normalize the intensity of the image\n",
    "    LoadImaged(keys=\"label\", image_only=True),\n",
    "    EnsureChannelFirstd(keys=\"label\"),\n",
    "]\n",
    "\n",
    "# Transforms to augment data\n",
    "range_x = [-15.0 / 180 * np.pi, 15.0 / 180 * np.pi]\n",
    "data_augmentation_transforms = [\n",
    "    RandRotated(\n",
    "        #keys=[\"image\", \"label\"],\n",
    "        keys=\"label\",\n",
    "        range_x=range_x,\n",
    "        range_y=0,\n",
    "        range_z=0,\n",
    "        #mode=[\"bicubic\", \"nearest\"],\n",
    "        mode=\"nearest\",\n",
    "        padding_mode=\"zeros\",\n",
    "        prob=0.2,\n",
    "    ),\n",
    "    RandZoomd(\n",
    "        #keys=[\"image\", \"label\"],\n",
    "        keys=\"label\",\n",
    "        min_zoom=0.7,\n",
    "        max_zoom=1.4,\n",
    "        #mode=[\"bicubic\", \"nearest\"],\n",
    "        mode=\"nearest\",\n",
    "        padding_mode=\"constant\",\n",
    "        #align_corners=None,\n",
    "        align_corners=None,\n",
    "        prob=0.2,\n",
    "    ),\n",
    "    #RandGaussianNoised(keys=[\"image\"], std=0.01, prob=0.15),\n",
    "    #RandGaussianSmoothd(\n",
    "    #    keys=[\"image\"],\n",
    "    #    sigma_x=(0.5, 1.15),\n",
    "    #    sigma_y=(0.5, 1.15),\n",
    "    #    prob=0.15,\n",
    "    #),\n",
    "    #RandScaleIntensityd(keys=[\"image\"], factors=0.3, prob=0.15),\n",
    "    #RandAdjustContrastd(keys=[\"image\"], gamma=(0.7, 1.5), prob=0.3),\n",
    "    #RandFlipd(keys=[\"image\", \"label\"], spatial_axis=[0], prob=0.5),\n",
    "    RandFlipd(keys=\"label\", spatial_axis=[0], prob=0.5),\n",
    "]\n",
    "\n",
    "# Define transforms for training, validation and testing\n",
    "train_transforms = Compose(load_transforms + data_augmentation_transforms)\n",
    "val_transforms = Compose(load_transforms)\n",
    "test_transforms = Compose(load_transforms)\n",
    "\n",
    "# Use CacheDataset to accelerate training and validation\n",
    "train_ds = CacheDataset(data=train_datalist, transform=train_transforms, cache_rate=1.0)\n",
    "val_ds = CacheDataset(data=val_datalist, transform=val_transforms, cache_rate=1.0)\n",
    "test_ds = CacheDataset(data=test_datalist, transform=test_transforms, cache_rate=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's visualize some images from the training set. The images are displayed with their corresponding ground truth segmentation masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_idx:  57\n",
      "val_idx:  186\n",
      "test_idx:  112\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from src.utils.visualizations import imagesc\n",
    "\n",
    "# Get a random index to display the image with label from each dataset\n",
    "train_idx = np.random.randint(len(train_ds))\n",
    "val_idx = np.random.randint(len(val_ds))\n",
    "test_idx = np.random.randint(len(test_ds))\n",
    "\n",
    "# Print the selected indices\n",
    "print(\"train_idx: \", train_idx)\n",
    "print(\"val_idx: \", val_idx)\n",
    "print(\"test_idx: \", test_idx)\n",
    "\n",
    "# Visualize a random image with label from each dataset\n",
    "colors = [\"black\", \"red\", \"green\", \"blue\"]\n",
    "cmap = ListedColormap(colors)\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "train_sample = train_ds[train_idx]\n",
    "#image = train_sample[\"image\"].detach().cpu().numpy()[0].transpose(1, 0)\n",
    "label = train_sample[\"label\"].detach().cpu().numpy()[0].transpose(1, 0)\n",
    "ax = figure.add_subplot(3, 2, 1)\n",
    "#imagesc(ax, image, title=\"Training image\", show_colorbar=False)\n",
    "#ax = figure.add_subplot(3, 2, 2)\n",
    "imagesc(\n",
    "    ax, label, title=\"Training label\", show_colorbar=False, colormap=cmap, interpolation=\"nearest\"\n",
    ")\n",
    "\n",
    "val_sample = val_ds[val_idx]\n",
    "#image = val_sample[\"image\"].detach().cpu().numpy()[0].transpose(1, 0)\n",
    "label = val_sample[\"label\"].detach().cpu().numpy()[0].transpose(1, 0)\n",
    "ax = figure.add_subplot(3, 2, 3)\n",
    "#imagesc(ax, image, title=\"Validation image\", show_colorbar=False)\n",
    "#ax = figure.add_subplot(3, 2, 4)\n",
    "imagesc(\n",
    "    ax,\n",
    "    label,\n",
    "    title=\"Validation label\",\n",
    "    show_colorbar=False,\n",
    "    colormap=cmap,\n",
    "    interpolation=\"nearest\",\n",
    ")\n",
    "\n",
    "test_sample = test_ds[test_idx]\n",
    "#image = test_sample[\"image\"].detach().cpu().numpy()[0].transpose(1, 0)\n",
    "label = test_sample[\"label\"].detach().cpu().numpy()[0].transpose(1, 0)\n",
    "ax = figure.add_subplot(3, 2, 5)\n",
    "#imagesc(ax, image, title=\"Test image\", show_colorbar=False)\n",
    "#ax = figure.add_subplot(3, 2, 6)\n",
    "imagesc(ax, label, title=\"Test label\", show_colorbar=False, colormap=cmap, interpolation=\"nearest\")\n",
    "figure.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Train <a class=\"anchor\" id=\"train\"></a>\n",
    "Let's move on to train a U-Net to segment the left ventricle, myocardium and left atrium. We will use the training and validation sets created in the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of U-Net architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================================================================================================\n",
       "Layer (type:depth-idx)                        Input Shape               Output Shape              Kernel Shape              Param #\n",
       "=================================================================================================================================================\n",
       "Auto_Encoder                                  [1, 1, 64, 64]            [1, 4, 64, 64]            --                        --\n",
       "├─_DoubleConv: 1-1                            [1, 1, 64, 64]            [1, 16, 64, 64]           --                        --\n",
       "│    └─Sequential: 2-1                        [1, 1, 64, 64]            [1, 16, 64, 64]           --                        --\n",
       "│    │    └─Conv2d: 3-1                       [1, 1, 64, 64]            [1, 16, 64, 64]           [3, 3]                    160\n",
       "│    │    └─BatchNorm2d: 3-2                  [1, 16, 64, 64]           [1, 16, 64, 64]           --                        32\n",
       "│    │    └─ReLU: 3-3                         [1, 16, 64, 64]           [1, 16, 64, 64]           --                        --\n",
       "│    │    └─Dropout: 3-4                      [1, 16, 64, 64]           [1, 16, 64, 64]           --                        --\n",
       "│    │    └─Conv2d: 3-5                       [1, 16, 64, 64]           [1, 16, 64, 64]           [3, 3]                    2,320\n",
       "│    │    └─BatchNorm2d: 3-6                  [1, 16, 64, 64]           [1, 16, 64, 64]           --                        32\n",
       "│    │    └─ReLU: 3-7                         [1, 16, 64, 64]           [1, 16, 64, 64]           --                        --\n",
       "│    │    └─Dropout: 3-8                      [1, 16, 64, 64]           [1, 16, 64, 64]           --                        --\n",
       "├─_Down: 1-2                                  [1, 16, 64, 64]           [1, 32, 32, 32]           --                        --\n",
       "│    └─Sequential: 2-2                        [1, 16, 64, 64]           [1, 32, 32, 32]           --                        --\n",
       "│    │    └─MaxPool2d: 3-9                    [1, 16, 64, 64]           [1, 16, 32, 32]           2                         --\n",
       "│    │    └─_DoubleConv: 3-10                 [1, 16, 32, 32]           [1, 32, 32, 32]           --                        14,016\n",
       "├─_Down: 1-3                                  [1, 32, 32, 32]           [1, 64, 16, 16]           --                        --\n",
       "│    └─Sequential: 2-3                        [1, 32, 32, 32]           [1, 64, 16, 16]           --                        --\n",
       "│    │    └─MaxPool2d: 3-11                   [1, 32, 32, 32]           [1, 32, 16, 16]           2                         --\n",
       "│    │    └─_DoubleConv: 3-12                 [1, 32, 16, 16]           [1, 64, 16, 16]           --                        55,680\n",
       "├─_Down: 1-4                                  [1, 64, 16, 16]           [1, 128, 8, 8]            --                        --\n",
       "│    └─Sequential: 2-4                        [1, 64, 16, 16]           [1, 128, 8, 8]            --                        --\n",
       "│    │    └─MaxPool2d: 3-13                   [1, 64, 16, 16]           [1, 64, 8, 8]             2                         --\n",
       "│    │    └─_DoubleConv: 3-14                 [1, 64, 8, 8]             [1, 128, 8, 8]            --                        221,952\n",
       "├─_Down: 1-5                                  [1, 128, 8, 8]            [1, 256, 4, 4]            --                        --\n",
       "│    └─Sequential: 2-5                        [1, 128, 8, 8]            [1, 256, 4, 4]            --                        --\n",
       "│    │    └─MaxPool2d: 3-15                   [1, 128, 8, 8]            [1, 128, 4, 4]            2                         --\n",
       "│    │    └─_DoubleConv: 3-16                 [1, 128, 4, 4]            [1, 256, 4, 4]            --                        886,272\n",
       "├─_Up: 1-6                                    [1, 256, 4, 4]            [1, 128, 8, 8]            --                        --\n",
       "│    └─ConvTranspose2d: 2-6                   [1, 256, 4, 4]            [1, 128, 8, 8]            [2, 2]                    131,200\n",
       "│    └─_DoubleConv: 2-7                       [1, 128, 8, 8]            [1, 128, 8, 8]            --                        --\n",
       "│    │    └─Sequential: 3-17                  [1, 128, 8, 8]            [1, 128, 8, 8]            --                        295,680\n",
       "├─_Up: 1-7                                    [1, 128, 8, 8]            [1, 64, 16, 16]           --                        --\n",
       "│    └─ConvTranspose2d: 2-8                   [1, 128, 8, 8]            [1, 64, 16, 16]           [2, 2]                    32,832\n",
       "│    └─_DoubleConv: 2-9                       [1, 64, 16, 16]           [1, 64, 16, 16]           --                        --\n",
       "│    │    └─Sequential: 3-18                  [1, 64, 16, 16]           [1, 64, 16, 16]           --                        74,112\n",
       "├─_Up: 1-8                                    [1, 64, 16, 16]           [1, 32, 32, 32]           --                        --\n",
       "│    └─ConvTranspose2d: 2-10                  [1, 64, 16, 16]           [1, 32, 32, 32]           [2, 2]                    8,224\n",
       "│    └─_DoubleConv: 2-11                      [1, 32, 32, 32]           [1, 32, 32, 32]           --                        --\n",
       "│    │    └─Sequential: 3-19                  [1, 32, 32, 32]           [1, 32, 32, 32]           --                        18,624\n",
       "├─_Up: 1-9                                    [1, 32, 32, 32]           [1, 16, 64, 64]           --                        --\n",
       "│    └─ConvTranspose2d: 2-12                  [1, 32, 32, 32]           [1, 16, 64, 64]           [2, 2]                    2,064\n",
       "│    └─_DoubleConv: 2-13                      [1, 16, 64, 64]           [1, 16, 64, 64]           --                        --\n",
       "│    │    └─Sequential: 3-20                  [1, 16, 64, 64]           [1, 16, 64, 64]           --                        4,704\n",
       "├─Conv2d: 1-10                                [1, 16, 64, 64]           [1, 4, 64, 64]            [1, 1]                    68\n",
       "=================================================================================================================================================\n",
       "Total params: 1,747,972\n",
       "Trainable params: 1,747,972\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 176.61\n",
       "=================================================================================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 9.11\n",
       "Params size (MB): 6.99\n",
       "Estimated Total Size (MB): 16.12\n",
       "================================================================================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "from src.models.autoencoder import Auto_Encoder\n",
    "\n",
    "input_channels = 1  # This is the number of input channels in the image\n",
    "input_shape = (input_channels, 64, 64)  # This is the shape of the input image to the network\n",
    "num_classes = 4  # This is the number of output classes\n",
    "output_shape = (num_classes, 64, 64)  # This is the shape of the output mask\n",
    "init_channels = 32  # This is the number of channels in the first layer of the network\n",
    "\n",
    "autoencoder = Auto_Encoder(input_shape=input_shape, output_shape=output_shape, init_channels=init_channels)\n",
    "\n",
    "# Print the summary of the network\n",
    "summary_kwargs = dict(\n",
    "    col_names=[\"input_size\", \"output_size\", \"kernel_size\", \"num_params\"], depth=3, verbose=0\n",
    ")\n",
    "summary(autoencoder, (1, *input_shape), device=\"cpu\", **summary_kwargs) #REMPLACER CUDA PAR CPU SI PROBLEMES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of optimizer and loss function\n",
    "We will use the Adam optimizer. The loss function is the L2, which is a standard loss function for auto-encoders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "\n",
    "#from src.utils.loss_functions.dice_loss import DC_and_CE_loss\n",
    "\n",
    "# Soft dice and CE loss function\n",
    "#loss_function = DC_and_CE_loss(\n",
    "#    {\"batch_dice\": True, \"smooth\": 1e-5, \"do_bg\": False}, weight_ce=1, weight_dice=1\n",
    "#)\n",
    "loss_function = torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = partial(torch.optim.Adam, params=autoencoder.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from typing import Union\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from monai.data import DataLoader\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.utils.tensor_utils import sum_tensor\n",
    "\n",
    "\n",
    "def train_process(\n",
    "    train_ds: Dataset,\n",
    "    val_ds: Dataset,\n",
    "    num_workers: int,\n",
    "    model: nn.Module,\n",
    "    loss_function: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    num_classes: int,\n",
    "    batch_size: int = 2,\n",
    "    lr: float = 0.001,\n",
    "    max_epochs: int = 30,\n",
    "    log_dir: Union[Path, str] = Path(\"../logs/camus_segmentation\"),\n",
    "    val_interval=1,\n",
    ") -> tuple[float, list[float], list[float], list[int], list[float]]:\n",
    "    \"\"\"Trains a neural network model for segmentation on the provided datasets using the specified parameters.\n",
    "\n",
    "    Args:\n",
    "        train_ds: Training dataset.\n",
    "        val_ds: Validation dataset.\n",
    "        num_workers: Number of workers to use for data loading.\n",
    "        model: Neural network model.\n",
    "        loss_function: Loss function.\n",
    "        optimizer: Optimizer.\n",
    "        num_classes: Number of classes to segment.\n",
    "        batch_size: Number of batch size. Defaults to 2.\n",
    "        lr: Learning rate. Defaults to 0.001.\n",
    "        max_epochs: Maximum training epochs. Defaults to 30.\n",
    "        log_dir: Path to the logging directory. Defaults to Path(\"../logs/camus_segmentation\").\n",
    "        val_interval: Epoch interval to perform evaluation steps. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        - Total time taken for training.\n",
    "        - List containing training loss values for each epoch.\n",
    "        - List containing validation loss values for each evaluation epoch.\n",
    "        - List containing epochs where validation is performed.\n",
    "        - List containing metric values for each evaluation epoch.\n",
    "    \"\"\"\n",
    "    # Create train and validation dataloaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    # Determine the device to run the model on\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"GPU detected, training on: {device}!\\n\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"GPU not detected, training on CPU!\\n\")\n",
    "\n",
    "    # Move the model to the device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Finalize the creation of the optimizer object with the learning rate\n",
    "    optimizer = optimizer(lr=lr)\n",
    "\n",
    "    # Convert log directory to Path object if needed\n",
    "    if not isinstance(log_dir, Path):\n",
    "        log_dir = Path(log_dir)\n",
    "    # Create the log directory if it does not exist\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Define some variables to keep track of the best metric values, epoch time and losses\n",
    "    best_metric = 0\n",
    "    best_metric_epoch = 0\n",
    "    epoch_train_loss_values = []\n",
    "    epoch_val_loss_values = []\n",
    "    metric_values = []\n",
    "    epoch_val = []\n",
    "    metric_per_class = {f\"metric/{i}\": [] for i in range(1, num_classes)}\n",
    "    total_start = time.time()\n",
    "\n",
    "    fit_pbar = tqdm(range(max_epochs), desc=\"Training\", unit=\"epoch\", position=0, leave=True)\n",
    "    pbar_metrics = {\"train/loss\": None, \"val/loss\": None, \"val/dice\": None}\n",
    "\n",
    "    for epoch in fit_pbar:\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_val_loss = 0\n",
    "        step = 0\n",
    "        epoch_tp_hard = []\n",
    "        epoch_fp_hard = []\n",
    "        epoch_fn_hard = []\n",
    "        for batch_data in train_dataloader:\n",
    "            step += 1\n",
    "            #inputs, labels = (\n",
    "            #    batch_data[\"image\"].to(device),\n",
    "            #    batch_data[\"label\"].to(device),\n",
    "            #)\n",
    "            labels = batch_data[\"label\"].to(device),\n",
    "            optimizer.zero_grad()\n",
    "            #outputs = model(inputs)\n",
    "            outputs = model(labels)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            pbar_metrics[\"loss\"] = loss.item()\n",
    "            pbar_metrics[\"train_batch\"] = f\"{step}/{len(train_dataloader)}\"\n",
    "            fit_pbar.set_postfix(pbar_metrics)\n",
    "\n",
    "        epoch_loss /= step\n",
    "        epoch_train_loss_values.append(epoch_loss)\n",
    "        pbar_metrics[\"train/loss\"] = epoch_loss\n",
    "        pbar_metrics.pop(\"loss\")\n",
    "        pbar_metrics.pop(\"train_batch\")\n",
    "        fit_pbar.set_postfix(pbar_metrics)\n",
    "\n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            step = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for val_data in val_dataloader:\n",
    "                    step += 1\n",
    "                    #val_inputs, val_labels = (\n",
    "                    #    val_data[\"image\"].to(device),\n",
    "                    #    val_data[\"label\"].to(device),\n",
    "                    #)\n",
    "                    val_labels = val_data[\"label\"].to(device)\n",
    "                    val_outputs = model(val_labels)\n",
    "                    val_loss = loss_function(val_outputs, val_labels)\n",
    "                    epoch_val_loss += val_loss.item()\n",
    "                    pbar_metrics[\"loss\"] = val_loss.item()\n",
    "                    pbar_metrics[\"val_batch\"] = f\"{step}/{len(val_dataloader)}\"\n",
    "                    fit_pbar.set_postfix(pbar_metrics)\n",
    "\n",
    "                    # Compute tp, fp, and fn that will be used to compute the hard dice metric per epoch\n",
    "                    val_labels = val_labels[:, 0]\n",
    "                    axes = tuple(range(1, len(val_labels.shape)))\n",
    "                    # Convert the output logits to a segmentation mask\n",
    "                    val_outputs = F.softmax(val_outputs, dim=1).argmax(dim=1)\n",
    "\n",
    "                    tp_hard = torch.zeros((val_labels.shape[0], num_classes - 1)).to(\n",
    "                        val_outputs.device.index\n",
    "                    )\n",
    "                    fp_hard = torch.zeros((val_labels.shape[0], num_classes - 1)).to(\n",
    "                        val_outputs.device.index\n",
    "                    )\n",
    "                    fn_hard = torch.zeros((val_labels.shape[0], num_classes - 1)).to(\n",
    "                        val_outputs.device.index\n",
    "                    )\n",
    "\n",
    "                    for c in range(1, num_classes):\n",
    "                        tp_hard[:, c - 1] = sum_tensor(\n",
    "                            (val_outputs == c).float() * (val_labels == c).float(), axes=axes\n",
    "                        )\n",
    "                        fp_hard[:, c - 1] = sum_tensor(\n",
    "                            (val_outputs == c).float() * (val_labels != c).float(), axes=axes\n",
    "                        )\n",
    "                        fn_hard[:, c - 1] = sum_tensor(\n",
    "                            (val_outputs != c).float() * (val_labels == c).float(), axes=axes\n",
    "                        )\n",
    "\n",
    "                    tp_hard = tp_hard.sum(0, keepdim=False).detach().cpu().numpy()\n",
    "                    fp_hard = fp_hard.sum(0, keepdim=False).detach().cpu().numpy()\n",
    "                    fn_hard = fn_hard.sum(0, keepdim=False).detach().cpu().numpy()\n",
    "\n",
    "                    # Store the tp_hard, fp_hard, and fn_hard per evaluation step\n",
    "                    epoch_tp_hard.append(list(tp_hard))\n",
    "                    epoch_fp_hard.append(list(fp_hard))\n",
    "                    epoch_fn_hard.append(list(fn_hard))\n",
    "\n",
    "                epoch_val_loss /= step\n",
    "                epoch_val_loss_values.append(epoch_val_loss)\n",
    "                epoch_val.append(epoch + 1)\n",
    "                pbar_metrics.pop(\"loss\")\n",
    "                pbar_metrics.pop(\"val_batch\")\n",
    "                pbar_metrics[\"val/loss\"] = float(epoch_val_loss)\n",
    "\n",
    "                # Compute the hard dice metric per epoch\n",
    "                epoch_tp_hard = np.sum(epoch_tp_hard, 0)\n",
    "                epoch_fp_hard = np.sum(epoch_fp_hard, 0)\n",
    "                epoch_fn_hard = np.sum(epoch_fn_hard, 0)\n",
    "\n",
    "                # Compute the hard dice metric per class\n",
    "                global_dc_per_class = [\n",
    "                    i if not np.isnan(i) else 0.0\n",
    "                    for i in [\n",
    "                        2 * i / (2 * i + j + k)\n",
    "                        for i, j, k in zip(epoch_tp_hard, epoch_fp_hard, epoch_fn_hard)\n",
    "                    ]\n",
    "                ]\n",
    "\n",
    "                val_metric = np.mean(global_dc_per_class)\n",
    "                metric_values.append(val_metric)\n",
    "                pbar_metrics[\"val/dice\"] = val_metric\n",
    "                for i in range(1, num_classes, 1):\n",
    "                    pbar_metrics[f\"val/dice/{i}\"] = global_dc_per_class[i - 1]\n",
    "                    metric_per_class[f\"metric/{i}\"].append(global_dc_per_class[i - 1])\n",
    "                fit_pbar.set_postfix(pbar_metrics)\n",
    "\n",
    "                if val_metric > best_metric:\n",
    "                    best_metric = val_metric\n",
    "                    best_metric_epoch = epoch + 1\n",
    "                    # Save best metric model checkpoint\n",
    "                    torch.save(\n",
    "                        {\n",
    "                            \"max_epochs\": max_epochs,\n",
    "                            \"current_epoch\": epoch + 1,\n",
    "                            \"best_metric_epoch\": best_metric_epoch,\n",
    "                            \"train_loss\": epoch_train_loss_values,\n",
    "                            \"val_loss\": epoch_val_loss_values,\n",
    "                            \"epoch_val\": epoch_val,\n",
    "                            \"metric_values\": metric_values,\n",
    "                            \"metric_per_class\": metric_per_class,\n",
    "                            \"model_state_dict\": model.state_dict(),\n",
    "                            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                        },\n",
    "                        str(log_dir / \"best_metric_model.pth\"),\n",
    "                    )\n",
    "\n",
    "        # Save last model checkpoint\n",
    "        torch.save(\n",
    "            {\n",
    "                \"max_epochs\": max_epochs,\n",
    "                \"current_epoch\": epoch + 1,\n",
    "                \"best_metric_epoch\": best_metric_epoch,\n",
    "                \"train_loss\": epoch_train_loss_values,\n",
    "                \"val_loss\": epoch_val_loss_values,\n",
    "                \"epoch_val\": epoch_val,\n",
    "                \"metric_values\": metric_values,\n",
    "                \"metric_per_class\": metric_per_class,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            },\n",
    "            str(log_dir / \"last_model.pth\"),\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        f\"train completed, best_metric: {best_metric:.4f}\"\n",
    "        f\" at epoch: {best_metric_epoch}\"\n",
    "        f\" total time: {(time.time() - total_start):.4f}\"\n",
    "    )\n",
    "    return (\n",
    "        time.time() - total_start,\n",
    "        epoch_train_loss_values,\n",
    "        epoch_val_loss_values,\n",
    "        epoch_val,\n",
    "        metric_values,\n",
    "        metric_per_class,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of training hyperparameters\n",
    "In this section, we will define the hyperparameters for training, such as the number of epochs, the learning rate, the batch size, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not detected, training on CPU!\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d173f4b537a44c62904e2b1de89c9b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/10 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (tuple, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!tuple!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!tuple!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 14\u001b[0m\n\u001b[1;32m      4\u001b[0m num_workers \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mcpu_count() \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Number of workers to use for data loading\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m (\n\u001b[1;32m      8\u001b[0m     total_time,\n\u001b[1;32m      9\u001b[0m     epoch_train_loss_values,\n\u001b[1;32m     10\u001b[0m     epoch_val_loss_values,\n\u001b[1;32m     11\u001b[0m     epoch_val,\n\u001b[1;32m     12\u001b[0m     metric_values,\n\u001b[1;32m     13\u001b[0m     metric_per_class,\n\u001b[0;32m---> 14\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_process\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_ds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoencoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../logs/camus_segmentation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 116\u001b[0m, in \u001b[0;36mtrain_process\u001b[0;34m(train_ds, val_ds, num_workers, model, loss_function, optimizer, num_classes, batch_size, lr, max_epochs, log_dir, val_interval)\u001b[0m\n\u001b[1;32m    114\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m#outputs = model(inputs)\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(outputs, labels)\n\u001b[1;32m    118\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Bureau/TDSI/PTI/Auto_Encoder/auto-encoder-PTI/notebooks/../src/models/autoencoder.py:77\u001b[0m, in \u001b[0;36mAuto_Encoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;124;03m\"\"\"Defines the computation performed at every call.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m        (N, ``out_channels``, H, W), Raw, unnormalized scores for each class in the input's segmentation.\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x1)\n\u001b[1;32m     79\u001b[0m     x3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x2)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Bureau/TDSI/PTI/Auto_Encoder/auto-encoder-PTI/notebooks/../src/models/autoencoder.py:121\u001b[0m, in \u001b[0;36m_DoubleConv.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (tuple, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!tuple!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!tuple!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2  # Number of batch size\n",
    "lr = 0.001  # Learning rate\n",
    "max_epochs = 10  # Number of epochs to train the model\n",
    "num_workers = os.cpu_count() - 1  # Number of workers to use for data loading\n",
    "\n",
    "# Train the model\n",
    "(\n",
    "    total_time,\n",
    "    epoch_train_loss_values,\n",
    "    epoch_val_loss_values,\n",
    "    epoch_val,\n",
    "    metric_values,\n",
    "    metric_per_class,\n",
    ") = train_process(\n",
    "    train_ds=train_ds,\n",
    "    val_ds=val_ds,\n",
    "    num_workers=num_workers,\n",
    "    model=autoencoder,\n",
    "    loss_function=loss_function,\n",
    "    optimizer=optimizer,\n",
    "    num_classes=num_classes,\n",
    "    batch_size=batch_size,\n",
    "    lr=lr,\n",
    "    max_epochs=max_epochs,\n",
    "    log_dir=\"../logs/camus_segmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Visualize learning curves and predictions <a class=\"anchor\" id=\"visualize\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training losses, validation losses and validation dice over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "trains_epoch = list(range(1, max_epochs + 1, 1))\n",
    "vals_epochs = epoch_val\n",
    "\n",
    "plt.figure(\"train\", (16, 8))\n",
    "ax = plt.subplot(1, 3, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))  # Set integer ticks for x-axis\n",
    "plt.plot(trains_epoch, epoch_train_loss_values, color=\"red\", label=\"Train\")\n",
    "plt.plot(vals_epochs, epoch_val_loss_values, color=\"blue\", label=\"Val\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "ax = plt.subplot(1, 3, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))  # Set integer ticks for x-axis\n",
    "plt.plot(vals_epochs, metric_values, color=\"green\")\n",
    "\n",
    "ax = plt.subplot(1, 3, 3)\n",
    "plt.title(\"Val Mean Dice per Class\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))  # Set integer ticks for x-axis\n",
    "legend_metric = [\"left ventricle\", \"myocardium\", \"left atrium\"]\n",
    "for i in range(1, num_classes, 1):\n",
    "    plt.plot(vals_epochs, metric_per_class[f\"metric/{i}\"], label=legend_metric[i - 1])\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the pre-trained model weight for 50 epochs and visualize learning curves and metrics over 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "# Path to the logging directory\n",
    "log_dir = \"../logs/camus_segmentation\"\n",
    "\n",
    "# Determine the device to run the model on\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Load the best model weight\n",
    "best_model = Path(log_dir) / \"best_model_50epochs.pth\"\n",
    "if not best_model.is_file():\n",
    "    import urllib.request\n",
    "\n",
    "    urllib.request.urlretrieve(\n",
    "        \"https://www.creatis.insa-lyon.fr/~bernard/camus/best_model_50epochs.pth\",\n",
    "        str(best_model),\n",
    "    )\n",
    "\n",
    "# Load the weight\n",
    "weight = torch.load(best_model, map_location=device)\n",
    "epoch_train_loss_values = weight[\"train_loss\"]\n",
    "epoch_val_loss_values = weight[\"val_loss\"]\n",
    "epoch_val = weight[\"epoch_val\"]\n",
    "metric_values = weight[\"metric_values\"]\n",
    "metric_per_class = weight[\"metric_per_class\"]\n",
    "max_epochs = weight[\"max_epochs\"]\n",
    "best_metric_epoch = weight[\"best_metric_epoch\"]\n",
    "\n",
    "# Plot the training losses, validation losses and validation dice over epochs\n",
    "trains_epoch = list(range(1, max_epochs + 1, 1))\n",
    "vals_epochs = epoch_val\n",
    "\n",
    "plt.figure(\"train\", (16, 8))\n",
    "ax = plt.subplot(1, 3, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))  # Set integer ticks for x-axis\n",
    "plt.plot(trains_epoch, epoch_train_loss_values, color=\"red\", label=\"Train\")\n",
    "plt.plot(vals_epochs, epoch_val_loss_values, color=\"blue\", label=\"Val\")\n",
    "# Add a vertical line at the best model epoch\n",
    "plt.axvline(best_metric_epoch, color=\"gray\", linestyle=\"--\")\n",
    "plt.text(\n",
    "    best_metric_epoch + 0.5,\n",
    "    min(epoch_train_loss_values) * 4 / 5,\n",
    "    f\"Best model epoch = {best_metric_epoch}\",\n",
    "    rotation=90,\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "ax = plt.subplot(1, 3, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))  # Set integer ticks for x-axis\n",
    "plt.plot(vals_epochs, metric_values, color=\"green\")\n",
    "plt.axvline(best_metric_epoch, color=\"gray\", linestyle=\"--\")\n",
    "\n",
    "ax = plt.subplot(1, 3, 3)\n",
    "plt.title(\"Val Mean Dice per Class\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))  # Set integer ticks for x-axis\n",
    "legend_metric = [\"left ventricle\", \"myocardium\", \"left atrium\"]\n",
    "for i in range(1, num_classes, 1):\n",
    "    plt.plot(vals_epochs, metric_per_class[f\"metric/{i}\"], label=legend_metric[i - 1])\n",
    "plt.axvline(best_metric_epoch, color=\"gray\", linestyle=\"--\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the pre-trained model weight into the U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True to use the pretrained model\n",
    "# Set to False to use the model you just trained\n",
    "use_pretrained = False\n",
    "\n",
    "if use_pretrained:\n",
    "    weight = torch.load(Path(log_dir) / \"best_model_50epochs.pth\", map_location=device)\n",
    "else:\n",
    "    weight = torch.load(Path(log_dir) / \"best_metric_model.pth\", map_location=device)\n",
    "\n",
    "# Load the weight into the U-Net\n",
    "autoencoder.load_state_dict(weight[\"model_state_dict\"])\n",
    "# Move the U-Net to the correct device\n",
    "autoencoder.to(device)\n",
    "# Put the U-Net in evaluation mode\n",
    "autoencoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the inference on the test set\n",
    "First, we create the dataloader from the test dataset we defined at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the test dataloader\n",
    "num_workers = os.cpu_count() - 1\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=1,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# Create an iterator to iterate over the test dataloader\n",
    "test_dataloader_iter = iter(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we perform the inference on a sample from the test dataset and plot the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on a test image\n",
    "batch_data = next(test_dataloader_iter)\n",
    "with torch.no_grad():\n",
    "    image = batch_data[\"image\"].to(device)\n",
    "    pred = autoencoder(image)\n",
    "    pred = F.softmax(pred, dim=1).argmax(dim=1)\n",
    "    pred = pred.squeeze().detach().cpu().numpy().transpose(1, 0)\n",
    "    label = batch_data[\"label\"].squeeze().detach().cpu().numpy().transpose(1, 0)\n",
    "    image = image.squeeze().detach().cpu().numpy().transpose(1, 0)\n",
    "\n",
    "# Plot the image, label and prediction\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "ax = figure.add_subplot(1, 3, 1)\n",
    "imagesc(ax, image, title=\"Image\", show_colorbar=False)\n",
    "ax = figure.add_subplot(1, 3, 2)\n",
    "imagesc(ax, label, title=\"Label\", show_colorbar=False, colormap=cmap, interpolation=\"nearest\")\n",
    "ax = figure.add_subplot(1, 3, 3)\n",
    "imagesc(ax, pred, title=\"Prediction\", show_colorbar=False, colormap=cmap, interpolation=\"nearest\")\n",
    "figure.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
