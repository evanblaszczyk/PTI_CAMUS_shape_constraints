{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hLeWtr2IffPZ",
    "outputId": "db120c6a-36e0-48da-8560-75b7e2405153"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tj7UgtZRqHUA"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/project_ae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ehV6izLWAKiQ",
    "outputId": "2787048b-7a69-4af6-8dad-ba065e800f80"
   },
   "outputs": [],
   "source": [
    "!pip install monai\n",
    "!pip install torchinfo\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OD5E16d0YaJb"
   },
   "source": [
    "# Convolutional Auto-encoder - Camus Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train, test and evaluate a Convolutional Auto-encoder to generate an output label exactly as input, so the segmented shape. The dataset is available on [CAMUS dataset](https://humanheart-project.creatis.insa-lyon.fr/database/#collection/6373703d73e9f0047faa1bc8).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIEP44Z9YaJh"
   },
   "source": [
    "# I. Install dependencies <a class=\"anchor\" id=\"install\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ia5-ox_3YaJi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture project_path_setup\n",
    "\n",
    "import sys\n",
    "\n",
    "if \"../\" in sys.path:\n",
    "    print(sys.path)\n",
    "else:\n",
    "    sys.path.append(\"../\")\n",
    "    print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VX1_sSaLYaJk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture packages_install\n",
    "\n",
    "# Make sure the repo's package and its dependencies are installed\n",
    "%pip install -e ../."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNoonUEWYaJk"
   },
   "source": [
    "# II. Dataset <a class=\"anchor\" id=\"dataset\"></a>\n",
    "\n",
    "Once the environment is successfully setup, download the CAMUS dataset by executing the following cell. The dataset will be downloaded to the `data/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "38976e6bf0bf41e4bb9b038bc97cd69b",
      "7ce99a6b309d46d59e29456fdf33486c",
      "c69289c1175247b4bbd768c413e0e04a",
      "48957380b5a442aaaa382e48e577f353",
      "954a91183ca344ed97488c181096bc5a",
      "c67a241f2d06435d96db655d884005a4",
      "53a7b5962eea476aaef761faf3ddd190",
      "c503bb0da4f54165aef5c26dca4cd195",
      "bf89ec4fad074f2bbbd22cf8cb742cc2",
      "9ccc076b2e67435582d76a557a7fdae7",
      "d59ca2b713824aeaae5473d8584cafce"
     ]
    },
    "id": "ZOYCMW4tYaJl",
    "outputId": "ee4cfa3a-5126-49cc-861f-0f52c976d47e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Make sure the data is downloaded and extracted where it should be\n",
    "if not Path(\"../data/camus_64\").is_dir():\n",
    "    import zipfile\n",
    "    from io import BytesIO\n",
    "    from urllib.request import urlopen\n",
    "\n",
    "    zipurl = \"https://www.creatis.insa-lyon.fr/~bernard/camus/camus_64.zip\"\n",
    "    with urlopen(zipurl) as zipresp:\n",
    "        with zipfile.ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "            for member in tqdm(\n",
    "                zfile.infolist(), desc=\"Downloading and extracting data\", position=0, leave=True\n",
    "            ):\n",
    "                try:\n",
    "                  zfile.extract(member, \"../data/\")\n",
    "                except zipfile.error as e:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_2Xkq-kYaJl"
   },
   "source": [
    "Data splitting into training, validation and testing sets (60% of the data for training, 20% for validation and 20% for testing). The split is done by patient ID, so that the same patient will not appear in different sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KUD5Hm2qYaJm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.utils.file_and_folder_operations import subdirs\n",
    "\n",
    "# Specify the data directory\n",
    "data_dir = Path(\"../data/camus_64\").resolve()\n",
    "\n",
    "# List all the patients id\n",
    "keys = subdirs(data_dir, prefix=\"patient\", join=False)\n",
    "\n",
    "# Split the patients into 80/10/10 train/val/test sets\n",
    "train_keys, val_and_test_keys = train_test_split(keys, train_size=0.6, random_state=12345)\n",
    "val_keys, test_keys = train_test_split(val_and_test_keys, test_size=0.5, random_state=12345)\n",
    "\n",
    "train_keys = sorted(train_keys)\n",
    "val_keys = sorted(val_keys)\n",
    "test_keys = sorted(test_keys)\n",
    "\n",
    "# Create train, val and test datalist\n",
    "viws_instants = [\"2CH_ED\", \"2CH_ES\", \"4CH_ED\", \"4CH_ES\"]\n",
    "train_datalist = [\n",
    "    {\n",
    "        \"label\": str(data_dir / key / f\"{key}_{view}_gt.nii.gz\"),\n",
    "    }\n",
    "    for key in train_keys\n",
    "    for view in viws_instants\n",
    "]\n",
    "\n",
    "val_datalist = [\n",
    "    {\n",
    "        \"label\": str(data_dir / key / f\"{key}_{view}_gt.nii.gz\"),\n",
    "    }\n",
    "    for key in val_keys\n",
    "    for view in viws_instants\n",
    "]\n",
    "\n",
    "test_datalist = [\n",
    "    {\n",
    "        \"label\": str(data_dir / key / f\"{key}_{view}_gt.nii.gz\"),\n",
    "    }\n",
    "    for key in test_keys\n",
    "    for view in viws_instants\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ukbt3OI6f2Ya"
   },
   "source": [
    "Create a new function in order to convert one channel label to four channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dL8k2zLRgDeF"
   },
   "outputs": [],
   "source": [
    "from monai.transforms import MapTransform\n",
    "class ConvertToFourChannels(MapTransform):\n",
    "    def __init__(self, keys):\n",
    "        super().__init__(keys)\n",
    "\n",
    "    def __call__(self, data):\n",
    "        image = data[self.keys[0]]\n",
    "        four_channel_image = np.zeros((4, 64, 64), dtype=np.float32)\n",
    "        for i in range(4):\n",
    "            four_channel_image[i, image[0] == i] = 1.0\n",
    "            four_channel_image[i, image[0] != i] = 0.0\n",
    "        data[self.keys[0]] = four_channel_image\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8YdoovcYaJo"
   },
   "source": [
    "Once the data is split, we will create a `Dataset` object for each set. This object will be used to load the data during training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L0oGUq79YaJo",
    "outputId": "a6c8a528-f9bc-4781-dd03-651377a956f9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from monai.data import CacheDataset, meta_tensor\n",
    "from monai.transforms import Compose, EnsureChannelFirstd, LoadImaged, RandFlipd, RandRotated, RandZoomd, Lambda, ToTensor\n",
    "\n",
    "# Transforms to load data\n",
    "load_transforms = [\n",
    "    LoadImaged(keys=[\"label\"], image_only=True),  # Load label\n",
    "    EnsureChannelFirstd(keys=[\"label\"]),  # Make sure the first dimension is the channel dimension\n",
    "    ConvertToFourChannels(keys=[\"label\"]),\n",
    "    ToTensor(),\n",
    "]\n",
    "\n",
    "range_x = [-15.0 / 180 * np.pi, 15.0 / 180 * np.pi]\n",
    "data_augmentation_transforms = [\n",
    "    RandRotated(\n",
    "        keys=\"label\",\n",
    "        range_x=range_x,\n",
    "        range_y=0,\n",
    "        range_z=0,\n",
    "        mode=\"nearest\",\n",
    "        padding_mode=\"zeros\",\n",
    "        prob=0.2,\n",
    "    ),\n",
    "    RandFlipd(keys=\"label\", spatial_axis=[0], prob=0.5),\n",
    "]\n",
    "\n",
    "# Define transforms for training, validation and testing\n",
    "train_transforms = Compose(load_transforms + data_augmentation_transforms)\n",
    "val_transforms = Compose(load_transforms)\n",
    "test_transforms = Compose(load_transforms)\n",
    "\n",
    "# Use CacheDataset to accelerate training and validation\n",
    "train_ds = CacheDataset(data=train_datalist, transform=train_transforms, cache_rate=1.0)\n",
    "val_ds = CacheDataset(data=val_datalist, transform=val_transforms, cache_rate=1.0)\n",
    "test_ds = CacheDataset(data=test_datalist, transform=test_transforms, cache_rate=1.0)\n",
    "\n",
    "train_sample = train_ds[0]\n",
    "val_sample = val_ds[0]\n",
    "test_sample = train_ds[0]\n",
    "print(\"\\n\")\n",
    "print(type(train_sample[\"label\"]))\n",
    "print(type(val_sample[\"label\"]))\n",
    "print(type(test_sample[\"label\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfSrwFEPgSZz"
   },
   "source": [
    "Convert four channel label to one channel for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMg-A7ph5FKa"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def ConvertToOneChannel(img):\n",
    "  argmax_img = torch.argmax(img,dim=0)\n",
    "  one_channel_img = argmax_img.unsqueeze(0)\n",
    "  result = one_channel_img.detach().cpu().numpy()[0].transpose(1, 0)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HaBAiA5WkvHa"
   },
   "source": [
    "Select one example from train,vaidation and test dataset for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 862
    },
    "id": "XstAaTpEkFPc",
    "outputId": "e743d6ad-6555-46da-df46-4299e7b87484"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from src.utils.visualizations import imagesc\n",
    "\n",
    "# Get a random index to display the example annotation from each dataset\n",
    "train_idx = np.random.randint(len(train_ds))\n",
    "val_idx = np.random.randint(len(val_ds))\n",
    "test_idx = np.random.randint(len(test_ds))\n",
    "\n",
    "# Print the selected indices\n",
    "print(\"train_idx: \", train_idx+1)\n",
    "print(\"val_idx: \", val_idx+1)\n",
    "print(\"test_idx: \", test_idx+1)\n",
    "\n",
    "# Visualize a random example from each dataset\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "train_sample = train_ds[train_idx]\n",
    "label = ConvertToOneChannel(train_sample[\"label\"])\n",
    "ax = figure.add_subplot(3, 1, 1)\n",
    "imagesc(ax, label, title=\"Training input\", show_colorbar=False)\n",
    "\n",
    "val_sample = val_ds[val_idx]\n",
    "label = ConvertToOneChannel(val_sample[\"label\"])\n",
    "ax = figure.add_subplot(3, 1, 2)\n",
    "imagesc(ax, label, title=\"Validation input\", show_colorbar=False)\n",
    "\n",
    "test_sample = test_ds[test_idx]\n",
    "label = ConvertToOneChannel(test_sample[\"label\"])\n",
    "ax = figure.add_subplot(3, 1, 3)\n",
    "imagesc(ax, label, title=\"Test input\", show_colorbar=False)\n",
    "figure.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pW0JTtI7k67d"
   },
   "source": [
    "Test input label's data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71SWmrGjdxg_",
    "outputId": "00c48318-3c47-40ad-cf29-e74805901e01"
   },
   "outputs": [],
   "source": [
    "#np.unique(label)\n",
    "img = test_ds[0]\n",
    "np.unique(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgDUhaBLYaJp"
   },
   "source": [
    "# III. Train <a class=\"anchor\" id=\"train\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVg_ZU9MYaJq"
   },
   "source": [
    "## Definition of Convolutional Auto-encoder architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlOjTTtE7VNK"
   },
   "source": [
    "Create a fully convolutional auto-encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HgjwZP17iXxO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels),\n",
    "            nn.Dropout2d(p=0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to modify the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "                DoubleConv(in_channels, out_channels),\n",
    "                nn.Dropout2d(p=0.2)\n",
    "            )\n",
    "        else:\n",
    "            self.up = nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout2d(p=0.2)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.up(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class CompressConv(nn.Module):\n",
    "    def __init__(self, in_channels, compress_channel):\n",
    "        super(CompressConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, compress_channel, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class DeCompressConv(nn.Module):\n",
    "    def __init__(self, compress_channel, out_channels):\n",
    "        super(DeCompressConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(compress_channel, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class AE_Conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, init_channel, compress_channel, bilinear=True):\n",
    "        super(AE_Conv, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.init_channel = init_channel\n",
    "        self.compress_channel = compress_channel\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(in_channels, init_channel)\n",
    "        self.down1 = Down(init_channel, init_channel*2)\n",
    "        self.down2 = Down(init_channel*2, init_channel*4)\n",
    "        self.down3 = Down(init_channel*4, init_channel*8)\n",
    "        self.down4 = Down(init_channel*8, init_channel*16)\n",
    "        self.compress = CompressConv(init_channel*16, compress_channel)\n",
    "        self.decompress = DeCompressConv(compress_channel, init_channel*16)\n",
    "        self.up4 = Up(init_channel*16, init_channel*8, bilinear)\n",
    "        self.up3 = Up(init_channel*8, init_channel*4, bilinear)\n",
    "        self.up2 = Up(init_channel*4, init_channel*2, bilinear)\n",
    "        self.up1 = Up(init_channel*2, init_channel, bilinear)\n",
    "        self.outc = OutConv(init_channel, out_channels)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.inc(x)\n",
    "        x = self.down1(x)\n",
    "        x = self.down2(x)\n",
    "        x = self.down3(x)\n",
    "        x = self.down4(x)\n",
    "        x = self.compress(x)\n",
    "        x = self.decompress(x)\n",
    "        x = self.up4(x)\n",
    "        x = self.up3(x)\n",
    "        x = self.up2(x)\n",
    "        x = self.up1(x)\n",
    "        x = self.outc(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "       for m in self.modules():\n",
    "           if isinstance(m, nn.Conv2d):\n",
    "               nn.init.kaiming_normal_(m.weight)\n",
    "               if m.bias is not None:\n",
    "                   nn.init.zeros_(m.bias)\n",
    "           elif isinstance(m, nn.BatchNorm2d):\n",
    "               nn.init.constant_(m.weight, 1)\n",
    "               nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJJLbsZPljzy"
   },
   "source": [
    "Create an AE instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CRODcoPkYaJq",
    "outputId": "743bb406-f0b3-4eff-acf1-40f86d3910b8"
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "import torch\n",
    "\n",
    "input_channel = 4  # This is the number of input channels in the image\n",
    "input_shape = (input_channel, 64, 64)  # This is the shape of the input image to the network\n",
    "output_channel = 4  # This is the number of output channel\n",
    "output_shape = (output_channel, 64, 64)  # This is the shape of the output image\n",
    "init_channel = 64  # This is the output channel's number of input convolution\n",
    "compress_channel = 16  # This is the channel's number of compress convolution at the end of bottleneck\n",
    "\n",
    "ae_conv = AE_Conv(input_channel, output_channel, init_channel, compress_channel)\n",
    "\n",
    "# Print the summary of the network\n",
    "summary_kwargs = dict(col_names=[\"input_size\", \"output_size\", \"kernel_size\", \"num_params\"], depth=3, verbose=0)\n",
    "summary(ae_conv, (1, *input_shape), device=\"cpu\", **summary_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yYhPRBb0sAd7",
    "outputId": "b66c44e5-3bfd-4a92-fe49-45190c1a0bbe"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Test the pixel value range of auto-encoder input and output\n",
    "random_image = np.random.rand(4, 64, 64)\n",
    "tensor_image = torch.FloatTensor(random_image)\n",
    "tensor_image = tensor_image.unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    ae_output = ae_conv(tensor_image)\n",
    "\n",
    "print(\"Random Image:\")\n",
    "print(random_image)\n",
    "\n",
    "print(\"\\nAE Output:\")\n",
    "print(ae_output.squeeze().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uWJr42dmYaJr"
   },
   "source": [
    "## Definition of training function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RQKZ2vlV0ZB"
   },
   "source": [
    "## Definition of Normalized Cross-Correlation (NCC) metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZY7QI5YjVmnO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ncc(x, y):\n",
    "    eps = 1e-5\n",
    "    x = x.flatten()\n",
    "    y = y.flatten()\n",
    "    mean_x = np.mean(x)\n",
    "    mean_y = np.mean(y)\n",
    "    numerator = np.sum((x - mean_x) * (y - mean_y))\n",
    "    denominator = np.sqrt(np.sum((x - mean_x)**2) * np.sum((y - mean_y)**2))\n",
    "    ncc = numerator / (denominator + 1e-5)\n",
    "    return ncc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmpI_DEbV_Wg"
   },
   "source": [
    "## Definition of training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NMoQPbd-YaJr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from typing import Union\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from monai.data import DataLoader\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "#from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "def train_process(\n",
    "    train_ds: Dataset,\n",
    "    val_ds: Dataset,\n",
    "    num_workers: int,\n",
    "    model: nn.Module,\n",
    "    loss_function: nn.Module,\n",
    "    loss_weights: torch.Tensor,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    sheduler: torch.optim.lr_scheduler._LRScheduler,\n",
    "    batch_size: int = 4,\n",
    "    max_epochs: int = 200,\n",
    "    log_dir: Union[Path, str] = Path(\"/content/drive/MyDrive/project_ae/logs/ae/ae_second_step\"),\n",
    "    val_interval=1,\n",
    ") -> tuple[float, list[float], list[float], list[int], list[float]]:\n",
    "    \"\"\"Trains a neural network model for segmentation on the provided datasets using the specified parameters.\n",
    "\n",
    "    Args:\n",
    "        train_ds: Training dataset.\n",
    "        val_ds: Validation dataset.\n",
    "        num_workers: Number of workers to use for data loading.\n",
    "        model: Neural network model.\n",
    "        loss_function: Loss function.\n",
    "        optimizer: Optimizer.\n",
    "        num_classes: Number of classes to segment.\n",
    "        batch_size: Number of batch size. Defaults to 2.\n",
    "        max_epochs: Maximum training epochs. Defaults to 30.\n",
    "        log_dir: Path to the logging directory. Defaults to Path(\"../logs/camus_segmentation\").\n",
    "        val_interval: Epoch interval to perform evaluation steps. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        - Total time taken for training.\n",
    "        - List containing training loss values for each epoch.\n",
    "        - List containing validation loss values for each evaluation epoch.\n",
    "        - List containing epochs where validation is performed.\n",
    "        - List containing metric values for each evaluation epoch.\n",
    "    \"\"\"\n",
    "    # Create train and validation dataloaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    # Determine the device to run the model on\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"GPU detected, training on: {device}!\\n\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"GPU not detected, training on CPU!\\n\")\n",
    "\n",
    "    # Move the model to the device\n",
    "    model = model.to(device)\n",
    "    loss_weights  =loss_weights.to(device)\n",
    "    # Convert log directory to Path object if needed\n",
    "    if not isinstance(log_dir, Path):\n",
    "        log_dir = Path(log_dir)\n",
    "    # Create the log directory if it does not exist\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Define some variables to keep track of the best metric values, epoch time and losses\n",
    "    best_metric = -1\n",
    "    best_metric_epoch = -1\n",
    "    epoch_train_loss_values = []\n",
    "    epoch_val_loss_values = []\n",
    "    metric_values = []\n",
    "    epoch_val = []\n",
    "    total_start = time.time()\n",
    "\n",
    "    accuracy_list = []\n",
    "    ncc_list = []\n",
    "\n",
    "    fit_pbar = tqdm(range(max_epochs), desc=\"Training\", unit=\"epoch\", position=0, leave=True)\n",
    "    pbar_metrics = {\"train/loss\": None, \"val/loss\": None, \"val/accuracy\": None, \"val/ncc\": None}\n",
    "\n",
    "    for epoch in fit_pbar:\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_val_loss = 0\n",
    "        epoch_acc = 0\n",
    "        epoch_ncc = 0\n",
    "        step = 0\n",
    "        for batch_data in train_dataloader:\n",
    "            step += 1\n",
    "            inputs = batch_data[\"label\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            weighted_inputs = inputs * loss_weights\n",
    "            weighted_outputs = outputs * loss_weights\n",
    "            loss = loss_function(weighted_outputs, weighted_inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            pbar_metrics[\"loss\"] = loss.item()\n",
    "            pbar_metrics[\"train_batch\"] = f\"{step}/{len(train_dataloader)}\"\n",
    "            fit_pbar.set_postfix(pbar_metrics)\n",
    "\n",
    "        epoch_loss /= step\n",
    "        epoch_train_loss_values.append(epoch_loss)\n",
    "        pbar_metrics[\"train/loss\"] = epoch_loss\n",
    "        pbar_metrics.pop(\"loss\")\n",
    "        pbar_metrics.pop(\"train_batch\")\n",
    "        fit_pbar.set_postfix(pbar_metrics)\n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            step = 0\n",
    "            acc_per_epoch = 0\n",
    "            ssim_per_epoch = 0\n",
    "            ncc_per_epoch = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for val_data in val_dataloader:\n",
    "                    step += 1\n",
    "                    val_inputs = val_data[\"label\"].to(device)\n",
    "\n",
    "                    val_outputs = model(val_inputs)\n",
    "                    weighted_val_outputs = val_outputs * loss_weights\n",
    "                    weighted_val_inputs = val_inputs * loss_weights\n",
    "                    val_loss = loss_function(weighted_val_outputs, weighted_val_inputs)\n",
    "                    epoch_val_loss += val_loss.item()\n",
    "                    pbar_metrics[\"loss\"] = val_loss.item()\n",
    "                    pbar_metrics[\"val_batch\"] = f\"{step}/{len(val_dataloader)}\"\n",
    "                    fit_pbar.set_postfix(pbar_metrics)\n",
    "\n",
    "                    if val_inputs.size() != val_outputs.size():\n",
    "                      raise ValueError(\"Error!\")\n",
    "\n",
    "                    one_channel_val_outputs = torch.argmax(val_outputs, dim=1, keepdim=True)\n",
    "                    one_channel_val_inputs = torch.argmax(val_inputs, dim=1, keepdim=True)\n",
    "\n",
    "                    # # accuracy calculation for 1 batch without counting background\n",
    "                    # mask = (one_channel_val_inputs > 0).float()\n",
    "                    # masked_correct_pixel = torch.sum(torch.eq(one_channel_val_inputs, one_channel_val_outputs) * mask).item()\n",
    "                    # masked_total_pixel = torch.sum(mask).item()\n",
    "\n",
    "                    # acc = (masked_correct_pixel / masked_total_pixel) * 100\n",
    "                    # acc_per_epoch += acc\n",
    "\n",
    "                    # accuracy calculation for 1 batch\n",
    "                    correct_pixel = torch.sum(torch.eq(one_channel_val_inputs, one_channel_val_outputs)).item()\n",
    "                    total_pixel = one_channel_val_inputs.numel()\n",
    "                    acc = (correct_pixel / total_pixel) * 100\n",
    "                    acc_per_epoch += acc\n",
    "\n",
    "                    # # ssim calculation for 1 batch\n",
    "                    # ssim_per_sample = 0\n",
    "                    # out_np = one_channel_val_outputs.cpu().detach().numpy()\n",
    "                    # in_np = one_channel_val_inputs.cpu().detach().numpy()\n",
    "                    # for i in range(out_np.shape[0]):\n",
    "                    #     # if np.all(out_np[i, 0] == in_np[i, 0]):\n",
    "                    #     #     print(\"yes\")\n",
    "                    #     # else:\n",
    "                    #     #     print(\"no\")\n",
    "                    #     ssim_per_sample += ssim(out_np[i,0], in_np[i,0])\n",
    "                    #     print(ssim_per_sample)\n",
    "                    # ssim_per_epoch += ssim_per_sample / out_np.shape[0]\n",
    "\n",
    "                    # ncc calculation for 1 batch\n",
    "                    ncc_per_sample = 0\n",
    "                    out_np = one_channel_val_outputs.cpu().detach().numpy()\n",
    "                    in_np = one_channel_val_inputs.cpu().detach().numpy()\n",
    "                    for i in range(out_np.shape[0]):\n",
    "                        ncc_per_sample += ncc(out_np[i,0], in_np[i,0])\n",
    "                    ncc_per_epoch += ncc_per_sample / out_np.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "                epoch_val_loss /= step\n",
    "                epoch_val_loss_values.append(epoch_val_loss)\n",
    "                epoch_val.append(epoch + 1)\n",
    "                pbar_metrics.pop(\"loss\")\n",
    "                pbar_metrics.pop(\"val_batch\")\n",
    "                pbar_metrics[\"val/loss\"] = float(epoch_val_loss)\n",
    "\n",
    "                epoch_acc = acc_per_epoch / step\n",
    "                pbar_metrics[\"val/accuracy\"] = epoch_acc\n",
    "                fit_pbar.set_postfix(pbar_metrics)\n",
    "                accuracy_list.append(epoch_acc)\n",
    "\n",
    "                epoch_ncc = ncc_per_epoch / step\n",
    "                pbar_metrics[\"val/ncc\"] = epoch_ncc\n",
    "                fit_pbar.set_postfix(pbar_metrics)\n",
    "                ncc_list.append(epoch_ncc)\n",
    "\n",
    "                val_metric = epoch_acc\n",
    "                metric_values.append(val_metric)\n",
    "\n",
    "\n",
    "                if val_metric > best_metric:\n",
    "                    best_metric = val_metric\n",
    "                    best_metric_epoch = epoch + 1\n",
    "                    # Save best metric model checkpoint\n",
    "                    #torch.save(\n",
    "                    #    {\n",
    "                    #        \"max_epochs\": max_epochs,\n",
    "                    #        \"current_epoch\": epoch + 1,\n",
    "                    #        \"best_metric_epoch\": best_metric_epoch,\n",
    "                    #        \"train_loss\": epoch_train_loss_values,\n",
    "                    #        \"val_loss\": epoch_val_loss_values,\n",
    "                    #        \"epoch_val\": epoch_val,\n",
    "                    #        \"metric_values\": metric_values,\n",
    "                    #        \"model_state_dict\": model.state_dict(),\n",
    "                    #        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    #    },\n",
    "                    #    str(log_dir / \"best_metric_model_checkpoint_32.pth\"),\n",
    "                    #)\n",
    "\n",
    "                    # Save the best whole model\n",
    "                    torch.save(model, str(log_dir / \"best_metric_model_256.pth\"))\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Save last model checkpoint\n",
    "        # torch.save(\n",
    "        #     {\n",
    "        #         \"max_epochs\": max_epochs,\n",
    "        #         \"current_epoch\": epoch + 1,\n",
    "        #         \"best_metric_epoch\": best_metric_epoch,\n",
    "        #         \"train_loss\": epoch_train_loss_values,\n",
    "        #         \"val_loss\": epoch_val_loss_values,\n",
    "        #         \"epoch_val\": epoch_val,\n",
    "        #         \"metric_values\": metric_values,\n",
    "        #         \"model_state_dict\": model.state_dict(),\n",
    "        #         \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        #     },\n",
    "        #     str(log_dir / \"last_model_checkpoint.pth\"),\n",
    "        # )\n",
    "\n",
    "        # Save the whole model\n",
    "        # torch.save(model, str(log_dir / \"last_model.pth\"))\n",
    "\n",
    "    print(\n",
    "        f\"train completed, best_metric: {best_metric:.4f}\"\n",
    "        f\" at epoch: {best_metric_epoch}\"\n",
    "        f\" total time: {(time.time() - total_start):.4f}\"\n",
    "    )\n",
    "    return (\n",
    "        time.time() - total_start,\n",
    "        epoch_train_loss_values,\n",
    "        epoch_val_loss_values,\n",
    "        epoch_val,\n",
    "        metric_values,\n",
    "        accuracy_list,\n",
    "        ncc_list\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlph2ox1YaJq"
   },
   "source": [
    "## Definition of optimizer and loss function\n",
    "We will define the parameters and hyperparameters for training, such as the number of epochs, the learning rate, the batch size, etc. We will also use the Adam optimizer. The loss function is the MSE loss with weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tWdzmufoYaJr"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import torch\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "batch_size = 4  # Number of batch size\n",
    "max_epochs = 50  # Number of epochs to train the model\n",
    "num_workers = os.cpu_count() - 1  # Number of workers to use for data loading\n",
    "\n",
    "sample = train_ds[0]\n",
    "sample_size = sample[\"label\"].shape\n",
    "channels = sample_size[0]\n",
    "height = sample_size[1]\n",
    "width = sample_size[2]\n",
    "loss_weights = torch.ones((batch_size, channels, height, width))\n",
    "loss_weights[:, 1:] *= 3\n",
    "\n",
    "loss_function = MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(ae_conv.parameters(),lr=0.01,betas=(0.9,0.999),eps=1e-04,weight_decay=0.0005,amsgrad=False)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: 0.01 if epoch < 5 else (0.005 if 5 <= epoch < 20 else 0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znI-spjnYaJs"
   },
   "source": [
    "#IV. Training\n",
    "In this section, we will train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "wQVlLLiSYaJs",
    "outputId": "f3c52e88-f6aa-454c-c2c0-8f3d8e97f8b8"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "(\n",
    "    total_time,\n",
    "    epoch_train_loss_values,\n",
    "    epoch_val_loss_values,\n",
    "    epoch_val,\n",
    "    metric_values,\n",
    "    accuracy_list,\n",
    "    ncc_list\n",
    ") = train_process(\n",
    "    train_ds=train_ds,\n",
    "    val_ds=val_ds,\n",
    "    num_workers=num_workers,\n",
    "    model=ae_conv,\n",
    "    loss_function=loss_function,\n",
    "    loss_weights=loss_weights,\n",
    "    optimizer=optimizer,\n",
    "    sheduler=scheduler,\n",
    "    batch_size=batch_size,\n",
    "    max_epochs=max_epochs,\n",
    "    log_dir=\"/content/drive/MyDrive/project_ae/logs/ae/ae_second_step\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inGqLqLEYaJs"
   },
   "source": [
    "#V. Visualize learning curves and predictions <a class=\"anchor\" id=\"visualize\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xh5fOv6sYaJs"
   },
   "source": [
    "### Plot training losses, validation losses and validation SSIM over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 769
    },
    "id": "Fa-cF2UHYaJt",
    "outputId": "146b613c-618c-422c-bb65-01a95e0621f9"
   },
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "trains_epoch = list(range(1, max_epochs + 1, 1))\n",
    "vals_epochs = epoch_val\n",
    "\n",
    "plt.figure(\"train\", (16, 8))\n",
    "plt.suptitle(\"Train History\", x=0.5, y=0.95, fontsize=16, fontweight='bold')\n",
    "ax = plt.subplot(1, 3, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))  # Set integer ticks for x-axis\n",
    "plt.plot(trains_epoch, epoch_train_loss_values, color=\"red\", label=\"Train\")\n",
    "plt.plot(vals_epochs, epoch_val_loss_values, color=\"blue\", label=\"Val\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "ax = plt.subplot(1, 3, 2)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))  # Set integer ticks for x-axis\n",
    "plt.plot(vals_epochs, accuracy_list, color=\"green\", label=\"Val\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "ax = plt.subplot(1, 3, 3)\n",
    "plt.title(\"NCC\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))  # Set integer ticks for x-axis\n",
    "plt.plot(vals_epochs, ncc_list, color=\"violet\", label=\"Val\")\n",
    "plt.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7UfcpOGYaJu"
   },
   "source": [
    "### Prediction on the test set\n",
    "First, we create the dataloader from the test dataset we defined at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pVK4nedDYaJu"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Create the test dataloader\n",
    "num_workers = os.cpu_count() - 1\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=1,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# Create an iterator to iterate over the test dataloader\n",
    "test_dataloader_iter = iter(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qBpUPTDYaJu"
   },
   "source": [
    "Then, we perform the inference on a random sample from the test dataset and plot the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "id": "P71W6vnGYaJu",
    "outputId": "aebc143c-c91e-4aca-a012-73c177e84456",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "batch_data = next(test_dataloader_iter)\n",
    "model = torch.load('/content/drive/MyDrive/project_ae/logs/ae/ae_second_step/best_metric_model_16.pth', map_location=torch.device('cpu'))\n",
    "#model = torch.load('/content/drive/MyDrive/project_ae/logs/ae/ae_second_step/best_metric_model_16.pth')\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    label = batch_data[\"label\"].to(device)\n",
    "    output = model(label)\n",
    "\n",
    "    one_channel_test_outputs = torch.argmax(output, dim=1, keepdim=True)\n",
    "    one_channel_test_inputs = torch.argmax(label, dim=1, keepdim=True)\n",
    "\n",
    "    one_channel_test_outputs = one_channel_test_outputs.squeeze().detach().cpu().numpy().transpose(1, 0)\n",
    "    one_channel_test_inputs = one_channel_test_inputs.squeeze().detach().cpu().numpy().transpose(1, 0)\n",
    "\n",
    "# Plot the input label, output label\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "figure.suptitle(\"Random Sample\", x=0.5, y=0.7, fontsize=16, fontweight='bold')\n",
    "ax = figure.add_subplot(1, 2, 1)\n",
    "imagesc(ax, one_channel_test_inputs, title=\"Input label\", show_colorbar=False)\n",
    "ax = figure.add_subplot(1, 2, 2)\n",
    "imagesc(ax, one_channel_test_outputs, title=\"Output label\", show_colorbar=False)\n",
    "figure.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HgoQ_EKIWoVQ"
   },
   "source": [
    "Finally, we perform the inference on the whole test dataset and plot the result for the best and the worst prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GMZNxI7gYCfY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model = torch.load('/content/drive/MyDrive/project_ae/logs/ae/ae_second_step/best_metric_model_16.pth', map_location=torch.device('cpu'))\n",
    "#model = torch.load('/content/drive/MyDrive/project_ae/logs/ae/ae_second_step/best_metric_model_16.pth')\n",
    "\n",
    "test_loss_function = MSELoss()\n",
    "test_loss_list = []\n",
    "\n",
    "test_accuracy_list = []\n",
    "test_ncc_list = []\n",
    "\n",
    "test_step = 0\n",
    "max_acc = 0.0\n",
    "min_acc = 100.0\n",
    "max_ncc = 0.0\n",
    "min_ncc = 1.0\n",
    "max_acc_sample = None\n",
    "min_acc_sample = None\n",
    "max_ncc_sample = None\n",
    "min_ncc_sample = None\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_batch_data in test_dataloader:\n",
    "        test_step += 1\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        test_inputs = test_batch_data[\"label\"].to(device)\n",
    "        test_outputs = model(test_inputs)\n",
    "        test_loss = test_loss_function(test_outputs, test_inputs)\n",
    "        test_loss_list.append(test_loss.item())\n",
    "\n",
    "        # accuracy calculation for test dataset\n",
    "        one_channel_test_outputs = torch.argmax(test_outputs, dim=1, keepdim=True)\n",
    "        one_channel_test_inputs = torch.argmax(test_inputs, dim=1, keepdim=True)\n",
    "\n",
    "        correct_pixel = torch.sum(torch.eq(one_channel_test_inputs, one_channel_test_outputs)).item()\n",
    "        total_pixel = one_channel_test_inputs.numel()\n",
    "        test_acc = (correct_pixel / total_pixel) * 100\n",
    "        test_accuracy_list.append(test_acc)\n",
    "\n",
    "        # ssim calculation for test dataset\n",
    "        test_out_np = one_channel_test_outputs.cpu().detach().numpy()\n",
    "        test_in_np = one_channel_test_inputs.cpu().detach().numpy()\n",
    "        test_ncc = ncc(test_out_np[0, 0], test_in_np[0, 0])\n",
    "        test_ncc_list.append(test_ncc)\n",
    "\n",
    "        # Save the sample with the highest accuracy\n",
    "        if test_acc > max_acc:\n",
    "            max_acc = test_acc\n",
    "            max_acc_sample = {\n",
    "                'idx': test_step,\n",
    "                'acc': max_acc,\n",
    "                'input': one_channel_test_inputs.squeeze().detach().cpu().numpy().transpose(1, 0),\n",
    "                'output': one_channel_test_outputs.squeeze().detach().cpu().numpy().transpose(1, 0)\n",
    "            }\n",
    "\n",
    "        # Save the sample with the lowest accuracy\n",
    "        if test_acc < min_acc:\n",
    "            min_acc = test_acc\n",
    "            min_acc_sample = {\n",
    "                'idx': test_step,\n",
    "                'acc': min_acc,\n",
    "                'input': one_channel_test_inputs.squeeze().detach().cpu().numpy().transpose(1, 0),\n",
    "                'output': one_channel_test_outputs.squeeze().detach().cpu().numpy().transpose(1, 0)\n",
    "            }\n",
    "\n",
    "        # Save the sample with the highest NCC\n",
    "        if test_ncc > max_ncc:\n",
    "            max_ncc = test_ncc\n",
    "            max_ncc_sample = {\n",
    "                'idx': test_step,\n",
    "                'ncc': max_ncc,\n",
    "                'input': one_channel_test_inputs.squeeze().detach().cpu().numpy().transpose(1, 0),\n",
    "                'output': one_channel_test_outputs.squeeze().detach().cpu().numpy().transpose(1, 0)\n",
    "            }\n",
    "\n",
    "        # Save the sample with the lowest NCC\n",
    "        if test_ncc < min_ncc:\n",
    "            min_ncc = test_ncc\n",
    "            min_ncc_sample = {\n",
    "                'idx': test_step,\n",
    "                'ncc': min_ncc,\n",
    "                'input': one_channel_test_inputs.squeeze().detach().cpu().numpy().transpose(1, 0),\n",
    "                'output': one_channel_test_outputs.squeeze().detach().cpu().numpy().transpose(1, 0)\n",
    "            }\n",
    "\n",
    "test_sample_idx = list(range(1, test_step + 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_Le609qkLptI",
    "outputId": "1a3fe336-32c2-4356-c26b-d47b2378e6ce"
   },
   "outputs": [],
   "source": [
    "plt.figure(\"Test\", (16, 8))\n",
    "plt.suptitle(\"Test History\", x=0.5, y=1.0, fontsize=20, fontweight='bold')\n",
    "ax = plt.subplot(1, 3, 1)\n",
    "plt.title(\"Test loss\")\n",
    "plt.xlabel(\"Sample index\")\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))  # Set integer ticks for x-axis\n",
    "plt.plot(test_sample_idx, test_loss_list, color=\"black\", label=\"Sample loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "ax = plt.subplot(1, 3, 2)\n",
    "plt.title(\"Test Accuracy\")\n",
    "plt.xlabel(\"Sample index\")\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))  # Set integer ticks for x-axis\n",
    "plt.plot(test_sample_idx, test_accuracy_list, color=\"black\", label=\"Sample accuracy\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "ax = plt.subplot(1, 3, 3)\n",
    "plt.title(\"Test NCC\")\n",
    "plt.xlabel(\"Sample index\")\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))  # Set integer ticks for x-axis\n",
    "plt.plot(test_sample_idx, test_ncc_list, color=\"black\", label=\"Sample NCC\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "# Plot the input label, output label, rounded output label\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "figure.suptitle(f\"Highest Accuracy Sample Index: {max_acc_sample['idx']} ({max_acc_sample['acc']})\", x=0.5, y=0.8, fontsize=16, fontweight='bold')\n",
    "ax = figure.add_subplot(1, 2, 1)\n",
    "imagesc(ax, max_acc_sample['input'], title=\"Input label\", show_colorbar=False)\n",
    "ax = figure.add_subplot(1, 2, 2)\n",
    "imagesc(ax, max_acc_sample['output'], title=\"Output label\", show_colorbar=False)\n",
    "figure.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "figure.suptitle(f\"Lowest Accuracy Sample Index: {min_acc_sample['idx']} ({min_acc_sample['acc']})\", x=0.5, y=0.8, fontsize=16, fontweight='bold')\n",
    "ax = figure.add_subplot(1, 2, 1)\n",
    "imagesc(ax, min_acc_sample['input'], title=\"Input label\", show_colorbar=False)\n",
    "ax = figure.add_subplot(1, 2, 2)\n",
    "imagesc(ax, min_acc_sample['output'], title=\"Output label\", show_colorbar=False)\n",
    "figure.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "figure.suptitle(f\"Highest NCC Sample Index: {max_ncc_sample['idx']} ({max_ncc_sample['ncc']})\", x=0.5, y=0.8, fontsize=16, fontweight='bold')\n",
    "ax = figure.add_subplot(1, 2, 1)\n",
    "imagesc(ax, max_ncc_sample['input'], title=\"Input label\", show_colorbar=False)\n",
    "ax = figure.add_subplot(1, 2, 2)\n",
    "imagesc(ax, max_ncc_sample['output'], title=\"Output label\", show_colorbar=False)\n",
    "figure.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "figure.suptitle(f\"Lowest NCC Sample Index: {min_ncc_sample['idx']} ({min_ncc_sample['ncc']})\", x=0.5, y=0.8, fontsize=16, fontweight='bold')\n",
    "ax = figure.add_subplot(1, 2, 1)\n",
    "imagesc(ax, min_ncc_sample['input'], title=\"Input label\", show_colorbar=False)\n",
    "ax = figure.add_subplot(1, 2, 2)\n",
    "imagesc(ax, min_ncc_sample['output'], title=\"Output label\", show_colorbar=False)\n",
    "figure.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vxKA--BiMyg"
   },
   "source": [
    "Draw a graphe to show the evolution of metric (accuracy) of different model for the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRvqvkdIx6vt"
   },
   "source": [
    "We apply all the models on the test set, calculate their accuracy and NCC. The final results are recorded in one table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5r4Nu88CzAZ7"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# model_16 = torch.load('/content/drive/MyDrive/project_ae/logs/ae/ae_second_step/best_metric_model_16.pth', map_location=torch.device('cpu'))\n",
    "# model_32 = torch.load('/content/drive/MyDrive/project_ae/logs/ae/ae_second_step/best_metric_model_32.pth', map_location=torch.device('cpu'))\n",
    "# model_48 = torch.load('/content/drive/MyDrive/project_ae/logs/ae/ae_second_step/best_metric_model_48.pth', map_location=torch.device('cpu'))\n",
    "# model_64 = torch.load('/content/drive/MyDrive/project_ae/logs/ae/ae_second_step/best_metric_model_64.pth', map_location=torch.device('cpu'))\n",
    "# model_128 = torch.load('/content/drive/MyDrive/project_ae/logs/ae/ae_second_step/best_metric_model_128.pth', map_location=torch.device('cpu'))\n",
    "# model_256 = torch.load('/content/drive/MyDrive/project_ae/logs/ae/ae_second_step/best_metric_model_256.pth', map_location=torch.device('cpu'))\n",
    "\n",
    "model_16 = torch.load('/content/drive/MyDrive/project_ae/logs/ae/ae_second_step/best_metric_model_16.pth')\n",
    "model_32 = torch.load('/content/drive/MyDrive/project_ae/logs/ae/ae_second_step/best_metric_model_32.pth')\n",
    "model_48 = torch.load('/content/drive/MyDrive/project_ae/logs/ae/ae_second_step/best_metric_model_48.pth')\n",
    "model_64 = torch.load('/content/drive/MyDrive/project_ae/logs/ae/ae_second_step/best_metric_model_64.pth')\n",
    "model_128 = torch.load('/content/drive/MyDrive/project_ae/logs/ae/ae_second_step/best_metric_model_128.pth')\n",
    "model_256 = torch.load('/content/drive/MyDrive/project_ae/logs/ae/ae_second_step/best_metric_model_256.pth')\n",
    "models = [model_16, model_32, model_48, model_64, model_128, model_256]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37vcimOymmjm"
   },
   "source": [
    "Count the parameters for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JW2hTi4UqNjg"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gFVksS6sBOby",
    "outputId": "db5f2275-7ca3-4405-b253-44a498f439ab"
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    size_model = count_parameters(model)\n",
    "    print(f\"Size:{size_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7u1_00omuwO"
   },
   "source": [
    "Then we calculate the metrics for different models, plot them in 3 graphes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uANy6cSFqWug"
   },
   "outputs": [],
   "source": [
    "test_loss_function = MSELoss()\n",
    "\n",
    "test_loss_list = np.zeros((len(models), len(test_dataloader)))\n",
    "test_accuracy_list = np.zeros((len(models), len(test_dataloader)))\n",
    "test_ncc_list = np.zeros((len(models), len(test_dataloader)))\n",
    "\n",
    "test_step = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_batch_data in test_dataloader:\n",
    "        test_step += 1\n",
    "        for i, model in enumerate(models):\n",
    "            model.eval()\n",
    "            model.to(device)\n",
    "            test_inputs = test_batch_data[\"label\"].to(device)\n",
    "            test_outputs = model(test_inputs)\n",
    "            test_loss = test_loss_function(test_outputs, test_inputs)\n",
    "            test_loss_list[i, test_step - 1] = test_loss.item()\n",
    "\n",
    "            # accuracy calculation for test dataset\n",
    "            one_channel_test_outputs = torch.argmax(test_outputs, dim=1, keepdim=True)\n",
    "            one_channel_test_inputs = torch.argmax(test_inputs, dim=1, keepdim=True)\n",
    "\n",
    "            correct_pixel = torch.sum(torch.eq(one_channel_test_inputs, one_channel_test_outputs)).item()\n",
    "            total_pixel = one_channel_test_inputs.numel()\n",
    "            test_acc = (correct_pixel / total_pixel) * 100\n",
    "            test_accuracy_list[i, test_step - 1] = test_acc\n",
    "\n",
    "            # ncc calculation for test dataset\n",
    "            test_out_np = one_channel_test_outputs.cpu().detach().numpy()\n",
    "            test_in_np = one_channel_test_inputs.cpu().detach().numpy()\n",
    "            test_ncc = ncc(test_out_np[0, 0], test_in_np[0, 0])\n",
    "            test_ncc_list[i, test_step - 1] = test_ncc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DysNrHmjnDG7"
   },
   "source": [
    "Sort the metric lists for better visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scmdfQqP5arJ"
   },
   "outputs": [],
   "source": [
    "test_loss_list = np.sort(test_loss_list, axis=1)\n",
    "test_accuracy_list = np.sort(test_accuracy_list, axis=1)\n",
    "test_ncc_list = np.sort(test_ncc_list, axis=1)\n",
    "test_sample_idx = list(range(1, test_step + 1, 1))\n",
    "models_name = [\"model_16\", \"model_32\", \"model_48\", \"model_64\", \"model_128\", \"model_256\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 792
    },
    "id": "HesJ7m6huVUc",
    "outputId": "bbf83d05-377a-46dc-8bd4-6e2a81bb42c5"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "plt.figure(\"Test\", (16, 8))\n",
    "plt.suptitle(\"Test History\", x=0.5, y=1.0, fontsize=20, fontweight='bold')\n",
    "\n",
    "# Create subplots for Test loss, Test Accuracy, and Test NCC\n",
    "for i, metric_list in enumerate([test_loss_list, test_accuracy_list, test_ncc_list], 1):\n",
    "    ax = plt.subplot(1, 3, i)\n",
    "    plt.title(f\"Test {'Loss' if i == 1 else 'Accuracy' if i == 2 else 'NCC'}\")\n",
    "    plt.xlabel(\"Sample index\")\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))  # Set integer ticks for x-axis\n",
    "\n",
    "    for j, model_metric in enumerate(metric_list):\n",
    "        model_name = [name for name, value in locals().items() if value is models[j]][0]\n",
    "        plt.plot(test_sample_idx, model_metric, label=f\"{model_name}\")\n",
    "\n",
    "    plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEp1qUqVnIn9"
   },
   "source": [
    "Calculate the mean value for each metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9K-JsCqjfXSb",
    "outputId": "9b5a2dad-6917-4402-ca22-75e9b0405466"
   },
   "outputs": [],
   "source": [
    "average_test_loss = np.mean(test_loss_list, axis=1)\n",
    "average_test_accuracy = np.mean(test_accuracy_list, axis=1)\n",
    "average_test_ncc = np.mean(test_ncc_list, axis=1)\n",
    "\n",
    "print(\"Average Test Loss per Model:\")\n",
    "print(average_test_loss)\n",
    "\n",
    "print(\"Average Test Accuracy per Model:\")\n",
    "print(average_test_accuracy)\n",
    "\n",
    "print(\"Average Test NCC per Model:\")\n",
    "print(average_test_ncc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CofWrSa-nM9q"
   },
   "source": [
    "Calcute the standard deviation for each metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rKPXgFq1pZEB",
    "outputId": "1ff05811-4871-4c16-dbee-db87edbf38d3"
   },
   "outputs": [],
   "source": [
    "test_loss_std = np.std(test_loss_list, axis=1)\n",
    "test_accuracy_std = np.std(test_accuracy_list, axis=1)\n",
    "test_ncc_std = np.std(test_ncc_list, axis=1)\n",
    "\n",
    "print(\"Standard Deviation of Test Loss per Model:\")\n",
    "print(test_loss_std)\n",
    "\n",
    "print(\"Standard Deviation of Test Accuracy per Model:\")\n",
    "print(test_accuracy_std)\n",
    "\n",
    "print(\"Standard Deviation of Test NCC per Model:\")\n",
    "print(test_ncc_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "EwbjkpApYzRV",
    "outputId": "aa5d73df-d9c5-43ba-943d-4b5b802fc1c3"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.errorbar(models_name, average_test_loss, yerr=test_loss_std, label='Test Loss', marker='s', linestyle='-', capsize=3)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.title('Evolution of Test Loss with Standard Deviation for Different Dimension')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylim(0, 0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "oYSgwe9zpVl7",
    "outputId": "85663cc7-dc8c-4dea-d995-cc5132cbd58e"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.errorbar(models_name, average_test_accuracy, yerr=test_accuracy_std, label='Test Accuracy', marker='o', linestyle='-', capsize=3)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Evolution of Test Accuracy with Standard Deviation for Different Dimension')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylim(95, 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "7WV2R3PSpZPk",
    "outputId": "93b404e0-cd9a-4791-a654-78f0b50f3913"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.errorbar(models_name, average_test_ncc, yerr=test_ncc_std, label='Test NCC', marker='^', linestyle='-', capsize=3)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Test NCC')\n",
    "plt.title('Evolution of Test NCC with Standard Deviation for Different Dimension')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylim(0.9, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7vQAhrNnT3M"
   },
   "source": [
    "Extract the encoder of AE for dimension reduction algorithme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Er1iakCJcREy"
   },
   "outputs": [],
   "source": [
    "def ae_model(x, model):\n",
    "        x = model.inc(x)\n",
    "        x = model.down1(x)\n",
    "        x = model.down2(x)\n",
    "        x = model.down3(x)\n",
    "        x = model.down4(x)\n",
    "        x = model.compress(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zAfFCRRMsKq8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def ConvertBatchToOneChannel(batch):\n",
    "    argmax_channel = torch.argmax(batch, dim=1, keepdim=True)\n",
    "    result = argmax_channel.permute(0, 2, 3, 1)\n",
    "    result_np = result.detach().cpu().numpy()\n",
    "    return result_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "122iDd1LnbED"
   },
   "source": [
    "t-SNE visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wu9KfLJhZRh8"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "#model = torch.load('/content/drive/MyDrive/project_ae/logs/ae/ae_second_step/best_metric_model_16.pth', map_location=torch.device('cpu'))\n",
    "model = torch.load('/content/drive/MyDrive/project_ae/logs/ae/ae_second_step/best_metric_model_16.pth')\n",
    "tsne_2 = TSNE(n_components=2)\n",
    "tsne_3 = TSNE(n_components=3)\n",
    "\n",
    "all_tsne_2 = []\n",
    "all_tsne_3 = []\n",
    "\n",
    "tsne_dataloader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=40,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "  for tsne_batch_data in tsne_dataloader:\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    label = tsne_batch_data[\"label\"].to(device)\n",
    "    output = ae_model(label,model)\n",
    "    output = output.view(output.size(0), -1)\n",
    "\n",
    "    data_tsne_2 = tsne_2.fit_transform(output.cpu().numpy())\n",
    "    data_tsne_3 = tsne_3.fit_transform(output.cpu().numpy())\n",
    "\n",
    "    all_tsne_2.append(torch.tensor(data_tsne_2, dtype=torch.float32))\n",
    "    all_tsne_3.append(torch.tensor(data_tsne_3, dtype=torch.float32))\n",
    "\n",
    "all_tsne_2 = torch.cat(all_tsne_2, dim=0)\n",
    "all_tsne_3 = torch.cat(all_tsne_3, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "Uy11w6pqe6JF",
    "outputId": "0d07e552-1680-4a47-c3aa-0c4549cc0d6c"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_2 = all_tsne_2[:, 0].numpy()\n",
    "y_2 = all_tsne_2[:, 1].numpy()\n",
    "\n",
    "plt.scatter(x_2, y_2, c='b', s=20)\n",
    "plt.title('2D t-SNE Visualization')\n",
    "plt.xlabel('t-SNE X')\n",
    "plt.ylabel('t-SNE Y')\n",
    "plt.xlim(-15, 15)\n",
    "plt.ylim(-15, 15)\n",
    "\n",
    "circle = plt.Circle((0, 0), 1, color='black', fill=False)\n",
    "plt.gca().add_patch(circle)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "id": "MjzFnJiqfSC9",
    "outputId": "1ea1e5f0-49f2-4334-8411-59c2c75f20d4"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "x_3 = all_tsne_3[:, 0].numpy()\n",
    "y_3 = all_tsne_3[:, 1].numpy()\n",
    "z_3 = all_tsne_3[:, 2].numpy()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x_3, y_3, z_3)\n",
    "\n",
    "ax.set_title('3D t-SNE Visualization')\n",
    "ax.set_xlabel('t-SNE X')\n",
    "ax.set_ylabel('t-SNE Y')\n",
    "ax.set_zlabel('t-SNE Z')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqB113H1nfkl"
   },
   "source": [
    "PCA visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X7EatZzBJU2q"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "#model = torch.load('/content/drive/MyDrive/project_ae/logs/ae/ae_second_step/best_metric_model_16.pth', map_location=torch.device('cpu'))\n",
    "model = torch.load('/content/drive/MyDrive/project_ae/logs/ae/ae_second_step/best_metric_model_16.pth')\n",
    "\n",
    "pca_2 = PCA(n_components=2)\n",
    "\n",
    "all_pca_2 = []\n",
    "\n",
    "pca_dataloader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=40,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for pca_batch_data in pca_dataloader:\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "\n",
    "        label = pca_batch_data[\"label\"].to(device)\n",
    "        output = ae_model(label, model)\n",
    "        output = output.view(output.size(0), -1)\n",
    "\n",
    "        data_pca_2 = pca_2.fit_transform(output.cpu().numpy())\n",
    "\n",
    "        all_pca_2.append(torch.tensor(data_pca_2, dtype=torch.float32))\n",
    "\n",
    "all_pca_2 = torch.cat(all_pca_2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "otgGdIX6Jlcj",
    "outputId": "deb9129a-2922-4712-f583-5e6ff39ab932"
   },
   "outputs": [],
   "source": [
    "x_pca_2 = all_pca_2[:, 0].numpy()\n",
    "y_pca_2 = all_pca_2[:, 1].numpy()\n",
    "\n",
    "plt.scatter(x_pca_2, y_pca_2, c='b', s=20)\n",
    "plt.title('2D PCA Visualization')\n",
    "plt.xlabel('PCA X')\n",
    "plt.ylabel('PCA Y')\n",
    "\n",
    "circle = plt.Circle((0, 0), 1, color='black', fill=False)\n",
    "plt.gca().add_patch(circle)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujxICWmjQ1O8"
   },
   "source": [
    "# VI. Optimisation <a class=\"anchor\" id=\"optimisation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uj-c5HWGRQ4J"
   },
   "source": [
    "## Definition of Convolutional Auto-encoder Variant - Fully Connection Layer for latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JuOI9wT3Q9bz"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CompressLinear(nn.Module):\n",
    "    def __init__(self, in_channels, compress_channel):\n",
    "        super(CompressLinear, self).__init__()\n",
    "        self.fc_compress = nn.Linear(in_channels*4*4, compress_channel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc_compress(x)\n",
    "\n",
    "class DeCompressLinear(nn.Module):\n",
    "    def __init__(self, compress_channel, out_channels):\n",
    "        super(DeCompressLinear, self).__init__()\n",
    "        self.fc_decompress = nn.Linear(compress_channel, out_channels*4*4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc_decompress(x)\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Flatten,self).__init__()\n",
    "    self.flatten = nn.Flatten()\n",
    "\n",
    "  def forward(self,x):\n",
    "    return self.flatten(x)\n",
    "\n",
    "class LAE_Conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, init_channel, compress_channel, bilinear=True):\n",
    "        super(LAE_Conv, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.init_channel = init_channel\n",
    "        self.compress_channel = compress_channel\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(in_channels, init_channel)\n",
    "        self.down1 = Down(init_channel, init_channel*2)\n",
    "        self.down2 = Down(init_channel*2, init_channel*4)\n",
    "        self.down3 = Down(init_channel*4, init_channel*8)\n",
    "        self.down4 = Down(init_channel*8, init_channel*16)\n",
    "\n",
    "        self.compress = CompressLinear(init_channel*16, compress_channel)\n",
    "        self.decompress = DeCompressLinear(compress_channel, init_channel*16)\n",
    "\n",
    "        self.up4 = Up(init_channel*16, init_channel*8, bilinear)\n",
    "        self.up3 = Up(init_channel*8, init_channel*4, bilinear)\n",
    "        self.up2 = Up(init_channel*4, init_channel*2, bilinear)\n",
    "        self.up1 = Up(init_channel*2, init_channel, bilinear)\n",
    "        self.outc = OutConv(init_channel, out_channels)\n",
    "\n",
    "        self._initialize_weights()\n",
    "        self.flatten = Flatten()\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.inc(x)\n",
    "        x = self.down1(x)\n",
    "        x = self.down2(x)\n",
    "        x = self.down3(x)\n",
    "        x = self.down4(x)\n",
    "        x_flat = x.view(x.size(0), -1)\n",
    "        x_compress = self.leaky_relu(self.compress(x_flat))\n",
    "        x_decompress = self.leaky_relu(self.decompress(x_compress))\n",
    "        x_reshaped = x_decompress.view(x.size(0), x.size(1), 4, 4)\n",
    "        x = self.up4(x_reshaped)\n",
    "        x = self.up3(x)\n",
    "        x = self.up2(x)\n",
    "        x = self.up1(x)\n",
    "        x = self.outc(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "       for m in self.modules():\n",
    "           if isinstance(m, nn.Conv2d):\n",
    "               nn.init.kaiming_normal_(m.weight)\n",
    "               if m.bias is not None:\n",
    "                   nn.init.zeros_(m.bias)\n",
    "           elif isinstance(m, nn.BatchNorm2d):\n",
    "               nn.init.constant_(m.weight, 1)\n",
    "               nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XAZPI3IVcumz",
    "outputId": "1d64075b-cdb8-4333-8b7d-89438b4dfb93"
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "import torch\n",
    "\n",
    "input_channel = 4  # This is the number of input channels in the image\n",
    "input_shape = (input_channel, 64, 64)  # This is the shape of the input image to the network\n",
    "output_channel = 4  # This is the number of output channel\n",
    "output_shape = (output_channel, 64, 64)  # This is the shape of the output image\n",
    "init_channel = 64  # This is the output channel's number of input convolution\n",
    "compress_channel = 4  # This is the channel's number of compress convolution at the end of bottleneck\n",
    "\n",
    "lae_conv = LAE_Conv(input_channel, output_channel, init_channel, compress_channel)\n",
    "\n",
    "# Print the summary of the network\n",
    "summary_kwargs = dict(col_names=[\"input_size\", \"output_size\", \"kernel_size\", \"num_params\"], depth=3, verbose=0)\n",
    "summary(lae_conv, (1, *input_shape), device=\"cpu\", **summary_kwargs)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "mgDUhaBLYaJp",
    "iVg_ZU9MYaJq",
    "uWJr42dmYaJr",
    "5RQKZ2vlV0ZB",
    "MmpI_DEbV_Wg",
    "mlph2ox1YaJq",
    "znI-spjnYaJs",
    "inGqLqLEYaJs",
    "Xh5fOv6sYaJs"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "38976e6bf0bf41e4bb9b038bc97cd69b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7ce99a6b309d46d59e29456fdf33486c",
       "IPY_MODEL_c69289c1175247b4bbd768c413e0e04a",
       "IPY_MODEL_48957380b5a442aaaa382e48e577f353"
      ],
      "layout": "IPY_MODEL_954a91183ca344ed97488c181096bc5a"
     }
    },
    "48957380b5a442aaaa382e48e577f353": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9ccc076b2e67435582d76a557a7fdae7",
      "placeholder": "​",
      "style": "IPY_MODEL_d59ca2b713824aeaae5473d8584cafce",
      "value": " 6000/6000 [00:02&lt;00:00, 2954.09it/s]"
     }
    },
    "53a7b5962eea476aaef761faf3ddd190": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7ce99a6b309d46d59e29456fdf33486c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c67a241f2d06435d96db655d884005a4",
      "placeholder": "​",
      "style": "IPY_MODEL_53a7b5962eea476aaef761faf3ddd190",
      "value": "Downloading and extracting data: 100%"
     }
    },
    "954a91183ca344ed97488c181096bc5a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ccc076b2e67435582d76a557a7fdae7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf89ec4fad074f2bbbd22cf8cb742cc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c503bb0da4f54165aef5c26dca4cd195": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c67a241f2d06435d96db655d884005a4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c69289c1175247b4bbd768c413e0e04a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c503bb0da4f54165aef5c26dca4cd195",
      "max": 6000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bf89ec4fad074f2bbbd22cf8cb742cc2",
      "value": 6000
     }
    },
    "d59ca2b713824aeaae5473d8584cafce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
